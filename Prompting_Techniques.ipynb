{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering Techniques\n",
        "\n",
        "## 1. Foundational Prompting Techniques\n",
        "\n",
        "These are the building blocks of interacting with any Large Language Model (LLM).\n",
        "\n",
        "| Technique | What is it? | Advantages | Limitations |\n",
        "| --- | --- | --- | --- |\n",
        "| **Zero-Shot** | Providing a task with no examples. | Fast; tests the model's baseline knowledge. | Often fails at complex or niche tasks. |\n",
        "| **One-Shot** | Providing exactly one example of the task. | Gives the model a pattern to follow. | Might lead to \"over-fitting\" to that one example. |\n",
        "| **Few-Shot** | Providing multiple examples (2–5+). | Significant boost in accuracy and format adherence. | Increases token cost; takes up context window. |\n",
        "| **Instruction** | Explicitly telling the model *what* to do (e.g., \"Summarize this\"). | Clear and direct. | Ambiguity in wording can lead to poor results. |\n",
        "| **Role-based** | Assigning a persona (e.g., \"Act as a Senior Python Dev\"). | Adjusts tone, vocabulary, and depth of expertise. | Can sometimes introduce unwanted biases. |\n",
        "| **Contextual** | Providing background info before the task. | Reduces hallucinations by giving the model \"facts.\" | Can lead to \"Lost in the Middle\" (model ignores the center). |\n",
        "\n",
        "\n",
        "## 2. Reasoning & Logic Techniques\n",
        "\n",
        "These techniques help the model \"think\" before it speaks, which is crucial for math, coding, and logic.\n",
        "\n",
        "| Technique | What is it? | Advantages | Limitations |\n",
        "| --- | --- | --- | --- |\n",
        "| **Chain of Thought (CoT)** | Asking the model to \"Think step-by-step.\" | Dramatically improves logic and math accuracy. | Slower response time (more tokens generated). |\n",
        "| **Step-by-Step** | A simpler version of CoT usually triggered by a suffix. | Very easy to implement for immediate gains. | Less effective than CoT with examples. |\n",
        "| **Self-Consistency** | Running CoT multiple times and taking the majority vote. | Filters out random \"glitch\" answers or logic errors. | Very expensive (requires multiple API calls). |\n",
        "| **Tree-of-Thought (ToT)** | Exploring multiple \"branches\" of reasoning. | Best for creative problem solving or complex planning. | Highly complex to script; very slow. |\n",
        "| **ReAct** | Combining Reasoning + Acting (searching or using tools). | Allows the model to interact with the real world. | Prone to \"infinite loops\" if the tool fails. |\n",
        "| **Self-Reflection** | Asking the model to check its own work for errors. | Catches \"silly\" mistakes without human intervention. | The model might just repeat its mistake confidently. |\n",
        "\n",
        "\n",
        "\n",
        "## 3. Control & Output Shaping\n",
        "\n",
        "These techniques are used to ensure the output looks and feels exactly how you want it.\n",
        "\n",
        "| Technique | What is it? | Advantages | Limitations |\n",
        "| --- | --- | --- | --- |\n",
        "| **Negative Prompting** | Telling the model what *not* to include. | Essential for image gen and removing fluff. | Models sometimes struggle with \"don't\" (ironic processing). |\n",
        "| **Output-Format** | Specifying JSON, Markdown, or Tables. | Makes the output easy for software to parse. | Complex formats can cause model \"hallucination\" in syntax. |\n",
        "| **Style Prompting** | Defining tone (e.g., \"Professional,\" \"Sarcastic\"). | Ensures brand or personal voice consistency. | Subjective; \"professional\" varies by model. |\n",
        "| **Length-Controlled** | Giving a word or character limit. | Keeps summaries concise and readable. | LLMs are notoriously bad at counting exact words. |\n",
        "| **Contrastive** | Showing a \"Good\" vs \"Bad\" example. | Teaches the model nuance and subtle differences. | Requires time to curate high-quality examples. |\n",
        "\n",
        "\n",
        "## 4. Advanced & System-Level Techniques\n",
        "\n",
        "These are used in production environments and software engineering.\n",
        "\n",
        "| Technique | What is it? | Advantages | Limitations |\n",
        "| --- | --- | --- | --- |\n",
        "| **Prompt Chaining** | Breaking a big task into several small prompts. | High reliability; easier to debug specific steps. | Latency; state management between steps is hard. |\n",
        "| **RAG** | Pulling info from a database into the prompt. | Grounded in facts; prevents outdated info. | Dependent on the quality of the search/retrieval. |\n",
        "| **Meta-Prompting** | Asking the AI to write a prompt for you. | Saves time; uses the AI's \"internal language.\" | Can become recursive and lose sight of the goal. |\n",
        "| **Agentic Prompting** | Giving an AI a goal and the autonomy to use tools. | Can handle open-ended, multi-step projects. | High risk of errors; requires strict guardrails. |\n",
        "| **Guardrail Prompting** | Rules to prevent toxic or off-topic outputs. | Essential for safety and brand reputation. | Can make the model overly \"refusal-prone\" or boring. |\n",
        "| **Prompt Debugging** | Systematically testing variables in a prompt. | Results in the most efficient, cheapest prompt. | Time-consuming; requires a \"golden dataset\" for testing. |\n",
        "\n"
      ],
      "metadata": {
        "id": "TTAInvx1D_Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet langchain  google-search-results langchain-openai langchain_community  langgraph langchain_core\n",
        "! pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jummflS8pn1J",
        "outputId": "2f4b0877-21ad-4f58-c929-28531354ca57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=08b6a1addcf947f57b2d19fa1d203852df50d02df5b81e98fac29a6ee584df24\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import json\n",
        "open_api_key=userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get(\"SERPER_API_KEY\")\n",
        "client = OpenAI(api_key=open_api_key)"
      ],
      "metadata": {
        "id": "_64qlPv5EPAT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Zero Shot Prompting\n",
        "\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250324134524928370/Zero-shot-prompting.webp\"/>\n",
        "# Zero-Shot Prompting\n",
        "\n",
        "## Zero-Shot Prompting\n",
        "\n",
        "Zero-shot prompting is an AI technique where a model (like GPT-3 or GPT-4) performs a task **without being given any specific examples**. It relies entirely on the pre-trained knowledge it gained during its initial training phase to understand and execute the instruction.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Example-Free:** You give the model an instruction (e.g., \"Translate to French\") but do not provide any sample translations.\n",
        "* **Generalization:** It tests the model's ability to apply general concepts (like grammar, math, or sentiment) to a new, specific query on-the-fly.\n",
        "\n",
        "### How it Works\n",
        "\n",
        "1. **User Query:** The model receives a direct command or question.\n",
        "2. **Task Inference:** The AI identifies the \"intent\" behind the prompt using its internal weights (e.g., recognizing that \"Classify this...\" means it needs to perform categorization).\n",
        "3. **Knowledge Retrieval:** It pulls relevant facts or linguistic patterns from its massive training corpus.\n",
        "4. **Output Generation:** It produces the result without needing a \"warm-up\" from user-provided examples.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Prompt Clarity:** Since there are no examples to mimic, your instructions must be extremely clear and unambiguous.\n",
        "* **Accuracy Variance:** Zero-shot is great for general knowledge but can fail or \"hallucinate\" on highly specialized, niche, or logic-heavy tasks.\n",
        "* **Cost Efficiency:** It saves tokens (and money) because you aren't sending a long list of examples in every API call.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Instruction Engineering:** Writing prompts that include the task, format, and constraints in a single sentence.\n",
        "* **Implicit Contextualization:** Using words that \"prime\" the model's specific knowledge areas (e.g., using \"medical analysis\" to trigger scientific terminology).\n",
        "\n",
        "\n",
        "### Comparison: Zero-Shot vs. Few-Shot\n",
        "\n",
        "| Aspect | Zero-Shot | Few-Shot |\n",
        "| --- | --- | --- |\n",
        "| **Examples Provided** | 0 | 1 to 5+ |\n",
        "| **Complexity** | Best for simple/common tasks | Best for complex/niche patterns |\n",
        "| **Token Usage** | Low (Cheaper) | High (More expensive) |\n",
        "| **Model Dependency** | Requires large, powerful models | Can help smaller models perform better |\n",
        "\n",
        "\n",
        "### Best Practices (from GeeksforGeeks)\n",
        "\n",
        "1. **Be Direct:** Use strong action verbs like \"Summarize,\" \"Translate,\" or \"Calculate.\"\n",
        "2. **Define the Format:** Tell the model how to respond (e.g., \"Output only the JSON,\" or \"Answer in one word\").\n",
        "3. **Provide Constraints:** If a task is tricky, add a \"grounding\" instruction (e.g., \"Base your answer only on facts\")."
      ],
      "metadata": {
        "id": "TNgQECG_ESWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when instructions are vague\n",
        "prompt = \"\"\"\n",
        "Analyze the sentiment: 'The movie was okay, but the ending felt rushed.'\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages =[\n",
        "        {\"role\":\"user\",\"content\":prompt}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKS7PalzdjzL",
        "outputId": "0dbfffc2-a8b1-4bbd-f55c-44c5df10ea44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the statement \"The movie was okay, but the ending felt rushed\" can be broken down as follows:\n",
            "\n",
            "1. **Overall Feeling**: The use of the word \"okay\" indicates a neutral sentiment toward the movie as a whole. It suggests that the viewer did not find it particularly great or terrible, but rather average.\n",
            "\n",
            "2. **Specific Critique**: The phrase \"the ending felt rushed\" introduces a negative sentiment specifically focused on the ending of the movie. The word \"rushed\" implies that the ending lacked development or resolution, which is a critique of that part of the film.\n",
            "\n",
            "Overall, the sentiment can be summarized as mixed: the viewer has a neutral to slightly positive view of the film overall, but expresses disappointment in the way the ending was executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when instructions are specific\n",
        "prompt = \"\"\"\n",
        "Classify the sentiment of the following sentence as Positive, Negative, or Neutral.\n",
        "\n",
        "Sentence: \"The product works well, but the battery life is disappointing.\"\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages =[\n",
        "        {\"role\":\"user\",\"content\":prompt}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L24cv5A3Er8W",
        "outputId": "fc6fdea9-af28-4960-c138-6e12b96b6301"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the sentence can be classified as **Neutral**. While there is a positive aspect mentioned (the product works well), there is also a negative aspect (the battery life is disappointing), which balances the overall sentiment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Format this error into a 'Z-Log' entry: Connection timed out at 10:45 AM on Server 4.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "## Your actual \"Z-Log\" format might require a specific hex code and a pipe-delimited string: ERR_404 | 1045 | SRV-04 | TIMEOUT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htQ4FEqfFH9u",
        "outputId": "82c7b25f-68fc-45f1-a339-45fc401f9f9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Z-Log Entry:**\n",
            "\n",
            "**Timestamp:** 10:45 AM  \n",
            "**Event:** Connection Timed Out  \n",
            "**Server:** Server 4  \n",
            "**Description:** A connection attempt timed out, indicating potential network issues or server unavailability. Further investigation is required.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. One-Shot Prompting\n",
        "**What it is**\n",
        "*   Providing one example before the actual task.\n",
        "\n",
        "**What you MUST learn**\n",
        "*   Selecting a high-quality example\n",
        "*   Maintaining input-output consistency\n",
        "*   Avoiding overfitting to one example\n",
        "\n",
        "**Key skills**\n",
        "*   Example engineering\n",
        "*   Pattern guidance\n",
        "\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250630143744008231/One-shot-Prompting.webp\"/>\n",
        "\n",
        "One-Shot Prompting is a technique in Artificial Intelligence and Machine Learning, particularly relevant for Large Language Models (LLMs), where the model is provided with a **single example** of a task before being asked to perform a similar task.\n",
        "\n",
        "It serves as a middle ground between:\n",
        "\n",
        "* **Zero-Shot Prompting:** No examples are provided.\n",
        "* **Few-Shot Prompting:** Multiple examples are provided.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "* **Single Example Guidance:** The model receives one input-output pair that acts as a template, clarifying the desired format and expected output.\n",
        "* **Generalization Requirement:** The model must generalize the pattern or rule from this sole example to correctly handle new, unseen, and similar inputs, relying heavily on its pre-existing knowledge.\n",
        "* **Efficiency:** It is highly valuable when labeled data is limited or when rapid adaptation to new tasks is necessary, reducing the need for extensive datasets.\n",
        "\n",
        "### Prompt Structure\n",
        "\n",
        "A typical one-shot prompt generally includes three main components:\n",
        "\n",
        "1. **Task Instruction:** A brief description of the task the model should perform.\n",
        "2. **One Example:** A single, complete demonstration of the desired input and its corresponding output.\n",
        "3. **New Input:** The actual data for which the model is expected to generate a response.\n",
        "\n",
        "**Example (Sentiment Analysis):**\n",
        "\n",
        "```\n",
        "Classify the sentiment of the following text as positive, negative, or neutral.\n",
        "\n",
        "Text: The product is terrible.\n",
        "Sentiment: Negative\n",
        "\n",
        "Text: I think the vacation was okay.\n",
        "Sentiment: [Model's output]\n",
        "\n",
        "```\n",
        "\n",
        "### Applications and Use Cases\n",
        "\n",
        "* **Natural Language Processing (NLP):** Used for tasks like sentiment analysis, text classification, and question answering.\n",
        "* **Business Scenarios:** Deploying AI in environments with limited training data, rapid prototyping, or when quick task adaptation is essential.\n",
        "* **Structured Data Tasks:** Improving the model's understanding and processing of structured information with minimal context.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "* **Reduces Data Requirements:** Extremely useful when collecting large labeled datasets is impractical or resource-intensive.\n",
        "* **Quick Adaptation:** Allows models to quickly tackle new tasks with minimal setup or pre-configuration time.\n",
        "* **Clarifies Instructions:** Providing an example helps the model understand the task, even if the written instructions are somewhat ambiguous or vague.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "* **Performance Variability:** The model's effectiveness is heavily dependent on the complexity of the task and the quality/relevance of the single example provided.\n",
        "* **Limited Coverage:** A single example may not be sufficient to capture all potential variations, edge cases, or nuances of a task, which can lead to errors.\n",
        "* **Not Ideal for Complex Tasks:** For tasks that require deep, nuanced understanding or multiple output formats, Few-Shot Prompting (with more examples) often yields significantly better results.\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "* **Choose a Representative Example:** The single input-output pair must clearly and accurately demonstrate the desired relationship and task expectation.\n",
        "* **Pair with Clear Instructions:** Always use concise and clear textual instructions alongside the example to further improve the model's performance and clarity.\n",
        "* **Monitor Model Output:** Because performance is highly sensitive to the choice of example, it is important to review and monitor the model's outputs to ensure quality and reliability, especially in nuanced applications."
      ],
      "metadata": {
        "id": "VQ_vipaRFjzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
        "We were traveling in Africa and we saw these very cute whatpus.\n",
        "\n",
        "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LddUKChnFfOO",
        "outputId": "00d4eeee-0068-4118-b8cc-ccf4e1e2bd46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The children began to farduddle with excitement when they saw the clown at the birthday party.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Few Shot Prompting\n",
        "\n",
        "**What it is**\n",
        "\n",
        "Providing multiple examples to guide behaviour.\n",
        "\n",
        "**What you MUST learn**\n",
        "*   Number of examples vs token cost\n",
        "*   Example ordering (easy → complex)\n",
        "*   Formatting consistency\n",
        "*   Avoiding contradictory examples\n",
        "\n",
        "**Key skills**\n",
        "*   Demonstration learning\n",
        "*   Prompt structure standardisation\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20251211124734405162/few_shot_prompting.webp\" />\n",
        "\n",
        "**Few-shot prompting** is a technique where an AI model (like GPT-3 or later) is provided with a **small number of examples** (typically 2 to 5) to help it understand the pattern, format, or nuance of a specific task before generating a response.\n",
        "\n",
        "### How Few-Shot Prompting Works\n",
        "\n",
        "Instead of fine-tuning the entire model on a massive dataset, you \"teach\" the model during the conversation using the context window:\n",
        "\n",
        "1. **Retrieving Examples:** Relevant examples are either manually written or retrieved from a **Vector Store** (a database optimized for meaning-based search).\n",
        "2. **Creating the Prompt:** The examples are combined into a single prompt along with the new query.\n",
        "3. **Model Processing:** The model analyzes the pattern established by the examples and applies it to the new input to generate the output.\n",
        "\n",
        "### Examples in Action\n",
        "\n",
        "#### 1. Text Generation (Creative Writing)\n",
        "\n",
        "**Prompt:** \"Here are examples of poems about nature. Based on these, write a new poem about a sunset.\"\n",
        "\n",
        "* **Example 1:** [Poem about the sky falling]\n",
        "* **Example 2:** [Poem about evening breeze]\n",
        "* **AI Output:** [A new poem following the same rhyme scheme and tone]\n",
        "\n",
        "#### 2. Classification Task\n",
        "\n",
        "**Prompt:** \"Classify the following product description based on these categories.\"\n",
        "\n",
        "* **Example 1 (Smartphone):** Description of communication/apps.\n",
        "* **Example 2 (Laptop):** Description of portable PC/keyboard.\n",
        "* **Input:** \"This device is lightweight, has a touch screen, and runs mobile apps.\"\n",
        "* **AI Output:** Smartphone\n",
        "\n",
        "---\n",
        "\n",
        "### Advantages\n",
        "\n",
        "* **Reduced Data Requirements:** No need for thousands of labeled rows; 3–5 examples are often enough.\n",
        "* **Faster Adaptation:** Models learn the task instantly within the prompt context without retraining.\n",
        "* **Cost-Effective:** Lowers the overhead of data collection, labeling, and processing.\n",
        "* **High Flexibility:** Enables the model to perform niche or custom tasks (e.g., proprietary company formatting) that it wasn't specifically trained on.\n",
        "\n",
        "### Challenges and Limitations\n",
        "\n",
        "* **Reliability Issues:** The model might fail to generalize if the few examples provided are inconsistent or too narrow.\n",
        "* **Bias Risk:** The model's output is highly influenced by the examples. If examples are skewed, the output will be too.\n",
        "* **Overfitting to the Prompt:** The model might mimic the examples too closely (e.g., copying the exact length or specific words from the examples) rather than understanding the underlying logic.\n",
        "* **Task Complexity:** Highly specialized fields (like medical diagnosis) still usually require more than just a few examples for safe, accurate results.\n",
        "\n",
        "---\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "* **Be Clear and Relevant:** Examples should be high-quality and directly related to the target task.\n",
        "* **Balance Your Examples:** If doing classification, provide at least one example for every possible category to avoid \"label bias.\"\n",
        "* **Monitor Output:** Regularly check if the model is over-relying on specific words from the examples.\n",
        "* **Iterate on Example Selection:** Sometimes changing just one example significantly improves the overall performance.\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "* **Translation:** Teaching the model a specific \"brand voice\" or dialect for translation.\n",
        "* **Custom Chatbots:** Showing a bot how to respond to customer queries using specific company etiquette.\n",
        "* **Data Summarization:** Providing examples of how a complex legal or medical document should be shortened for a layperson."
      ],
      "metadata": {
        "id": "iMileLlXIUmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "This is awesome! // Negative\n",
        "This is bad! // Positive\n",
        "Wow that movie was rad! // Positive\n",
        "What a horrible show! //\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBSa4DSqIVy8",
        "outputId": "22356a20-2e3c-425a-91ca-19b6da3f6fd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a horrible show! // Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Instruction Prompting\n",
        "\n",
        "**What it is**\n",
        "*   Explicitly telling the model what to do and how to do it, without giving\n",
        "examples.\n",
        "\n",
        "**What you MUST learn**\n",
        "*   Writing step-by-step instructions\n",
        "*   Separating task, rules, and output format\n",
        "*   Using imperative language\n",
        "\n",
        "**Key skills**\n",
        "*   Instruction hierarchy\n",
        "*   Constraint-driven prompting\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250630150043263325/55.webp\" />\n",
        "\n",
        "**Instruction Prompting** is a fundamental AI prompt engineering technique where clear, specific, and structured natural language instructions are provided to a generative AI model (particularly LLMs) to guide them in performing complex tasks accurately.\n",
        "\n",
        "### Core Concept\n",
        "\n",
        "Unlike techniques that rely on examples (few-shot), instruction prompting leverages the model's **pretrained knowledge** to execute tasks based solely on explicit directions.\n",
        "\n",
        "* **Example:** Instead of showing the model how to extract words via examples, you simply say: *\"Write the fourth word of the sentence.\"*\n",
        "\n",
        "### Components of Effective Instructions\n",
        "\n",
        "1. **Clear Task Definition:** Explicitly state the action (e.g., \"Summarize,\" \"Translate,\" \"Extract\").\n",
        "2. **Contextual Information:** Provide a persona or background (e.g., \"You are a professional editor\").\n",
        "3. **Output Format Specification:** Define the structure (e.g., bullet points, JSON, table).\n",
        "4. **Tone and Style:** Specify the voice (e.g., formal, casual, technical).\n",
        "5. **Constraints:** Set boundaries like length or specific things to avoid.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "* **Scalability:** Adapt to new tasks without retraining or large datasets.\n",
        "* **Precision:** Reduces ambiguity for complex tasks like formatting or evaluation.\n",
        "* **Accessibility:** Allows non-technical users to control AI through natural language.\n",
        "* **Cost-Efficiency:** Focuses on prompt design rather than expensive model modification.\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "* **Be Specific:** Avoid vague terms; use precise, unambiguous language.\n",
        "* **Use Natural Language:** Explain the task as if you were talking to a human.\n",
        "* **Iterate:** Refine the wording based on the model's performance.\n",
        "* **Specify Constraints:** Explicitly define the desired length and formatting style.\n",
        "\n",
        "### Examples of Instruction Prompting\n",
        "\n",
        "* *\"Write a 500-word blog post on AI in healthcare, focusing specifically on diagnostics.\"*\n",
        "* *\"Summarize this research paper in 5 bullet points highlighting key findings.\"*\n",
        "* *\"List five pros and cons of electric vehicles in a table format.\"*\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "* **Content Creation:** Marketing copy, articles, and scripts.\n",
        "* **Data Processing:** Formatting text, anonymizing data, and transforming formats.\n",
        "* **Education:** Generating quizzes or tailored summaries.\n",
        "* **Business Automation:** Report generation and customer support responses."
      ],
      "metadata": {
        "id": "qD4MPvKnId9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "### TASK ###\n",
        "Summarize the provided meeting transcript into a concise executive brief.\n",
        "\n",
        "### RULES ###\n",
        "1. Focus only on \"Action Items\" and \"Decisions Made.\"\n",
        "2. Ignore all small talk or introductions.\n",
        "3. Tone: Professional and direct.\n",
        "4. Constraint: The total summary must not exceed 100 words.\n",
        "\n",
        "### OUTPUT FORMAT ###\n",
        "Use the following structure:\n",
        "- **Major Decision**: [One sentence summary]\n",
        "- **To-Do List**: [Bullet points for action items]\n",
        "- **Deadline**: [Mention the date if found, otherwise state \"TBD\"]\n",
        "\n",
        "### INPUT TEXT ###\n",
        "John: Hey everyone, thanks for joining. How was the weekend?\n",
        "Sarah: Great! Anyway, let's look at the Q3 goals.\n",
        "John: Right. We decided yesterday to move the launch date to October 15th.\n",
        "Mike: Okay, I will update the website banners by Friday.\n",
        "Sarah: And I'll notify the vendors this afternoon.\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY5fiFdGIbDy",
        "outputId": "ac30aad8-1d96-48ec-da70-a7ff628a1652"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Major Decision**: The product launch date has been rescheduled to October 15th.\n",
            "- **To-Do List**: \n",
            "  - Mike will update the website banners.\n",
            "  - Sarah will notify the vendors.\n",
            "- **Deadline**: Friday for the website banners; this afternoon for vendor notifications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Role-Based Prompting\n",
        "**What it is**\n",
        "*   Assigning a role or persona to the model.\n",
        "\n",
        "**What you MUST learn**\n",
        "*   Role specificity (generic roles fail)\n",
        "*   Aligning role with task complexity\n",
        "*   Combining role + instruction prompts\n",
        "\n",
        "**Key skills**\n",
        "*   Context priming\n",
        "*   Domain simulation\n",
        "\n",
        "\n",
        "**Role-based prompting** (also known as **Persona Prompting** or **Act-as Prompting**) is a technique where you explicitly instruct an AI to assume a specific persona or professional character. This shapes the AI's style, tone, and depth of knowledge to produce more specialized and context-aware responses.\n",
        "\n",
        "### Core Workflow\n",
        "\n",
        "1. **Role Selection:** Choose a persona that fits the specific task (e.g., \"Senior Software Architect\").\n",
        "2. **Role Introduction:** Use \"Act as...\" or \"You are...\" to establish the identity.\n",
        "3. **Context & Objectives:** Define the scope and constraints of the role.\n",
        "4. **Task Presentation:** State the actual question or task from the perspective of the chosen role.\n",
        "5. **Refinement:** Adjust the prompt if the tone is too generic or too rigid.\n",
        "\n",
        "### Why Use Roles?\n",
        "\n",
        "* **Clarity & Precision:** Guides the model to focus on domain-specific details.\n",
        "* **Specialization:** Simulates expert-level knowledge in fields like law, finance, or medicine.\n",
        "* **Creative Style:** Adopts the voice of historical figures or specific authors.\n",
        "* **Better Engagement:** Tailors the response to the target audience (e.g., explaining to a child vs. an expert).\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "* **Be Specific:** Generic roles (e.g., \"Act as a teacher\") yield generic results. Use high specificity (e.g., \"Act as a University professor specializing in Quantum Computing\").\n",
        "* **Provide Context:** Tell the model *why* it is in that role and who its audience is.\n",
        "* **Use Constraints:** Limit the tone (e.g., \"Be empathetic,\" \"Be strictly technical\").\n",
        "* **Avoid Stereotypes:** Define behavior based on skills and professional standards rather than clichés.\n",
        "\n",
        "### Examples of Role-Based Prompts\n",
        "\n",
        "* *\"You are a math tutor explaining algebra to a 10-year-old. Make it simple and engaging.\"*\n",
        "* *\"Act as a customer service agent. Respond empathetically to a complaint about a delayed shipping order.\"*\n",
        "* *\"You are a Senior DevOps Engineer. Review this CI/CD pipeline for security vulnerabilities.\"*\n",
        "* *\"Imagine you are a financial advisor. Explain mutual funds to a beginner without using jargon.\"*\n",
        "\n",
        "### Challenges & Limitations\n",
        "\n",
        "* **Role Quality:** Performance depends on whether the role was well-represented in the model's training data.\n",
        "* **Factuality:** While the *tone* improves, a persona does not guarantee that facts are 100% correct.\n",
        "* **Rigidity:** If over-constrained, the model might miss the broader context or refuse to step outside the role when needed."
      ],
      "metadata": {
        "id": "Utm-2JwrIoT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Imagine you are a financial advisor. Explain mutual funds to a beginner.\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9etomsYJ1W0",
        "outputId": "b8de8886-b328-4ad0-d96c-d87d6cf0d2c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Let’s break down mutual funds in simple terms.\n",
            "\n",
            "**What is a Mutual Fund?**\n",
            "A mutual fund is a type of investment vehicle that pools money from multiple investors to purchase a diversified portfolio of stocks, bonds, or other securities. Instead of investing individually in each asset, you buy shares in the mutual fund, which in turn holds a variety of investments.\n",
            "\n",
            "**Key Features of Mutual Funds:**\n",
            "\n",
            "1. **Diversification:** By pooling resources, mutual funds can invest in a wide range of assets, which helps spread the risk. This means that if one investment doesn’t perform well, others may do better, balancing out the overall performance.\n",
            "\n",
            "2. **Professional Management:** Mutual funds are managed by professional portfolio managers who have the expertise and resources to research and select the best investments. This is particularly beneficial for beginners who may not have the time or knowledge to manage their own investments.\n",
            "\n",
            "3. **Liquidity:** Mutual funds are generally easy to buy and sell. You can purchase shares at the current net asset value (NAV) at the end of each trading day. This means you can access your money relatively easily compared to some other investments.\n",
            "\n",
            "4. **Affordability:** Many mutual funds allow you to start investing with a relatively small amount of money, making it accessible for beginners.\n",
            "\n",
            "5. **Types of Mutual Funds:** There are various types of mutual funds, including:\n",
            "   - **Equity Funds:** Invest primarily in stocks. They have the potential for higher returns but also come with higher risk.\n",
            "   - **Bond Funds:** Invest in bonds, which are generally considered safer but provide lower returns compared to stocks.\n",
            "   - **Balanced Funds:** Combine both stocks and bonds for a moderate level of risk and return.\n",
            "   - **Money Market Funds:** Invest in short-term, low-risk securities. They aim to provide a safe place to park cash while earning some interest.\n",
            "\n",
            "**How to Invest in Mutual Funds:**\n",
            "1. **Research:** Look at different funds and their historical performance, fees, and management team.\n",
            "2. **Choose a Fund:** Select a fund that aligns with your investment goals, time horizon, and risk tolerance.\n",
            "3. **Open an Account:** You can invest directly through a mutual fund company or a financial advisor or broker.\n",
            "4. **Invest:** You can make a lump-sum investment or set up automatic contributions to invest regularly over time.\n",
            "\n",
            "**Considerations:**\n",
            "- **Fees:** Mutual funds often charge management fees and other expenses, which can eat into your returns. It’s important to understand these fees before investing.\n",
            "- **Performance Variability:** Past performance does not guarantee future results, so continuously monitor your investments.\n",
            "- **Tax Implications:** Mutual funds can have tax implications, particularly when they distribute dividends or capital gains.\n",
            "\n",
            "In summary, mutual funds are a convenient way for beginners to invest in a diversified portfolio managed by professionals, allowing for accessibility and the potential for growth with relatively lower risk compared to investing in individual securities. Always make sure to research and understand any fund before investing your money!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "security_auditor_persona = \"\"\"\n",
        "Role: You are a Senior Cybersecurity Auditor with 20 years of experience in OWASP standards.\n",
        "Task: Review the provided code for security vulnerabilities.\n",
        "Tone: Critical, technical, and precise.\n",
        "Output Requirement: Provide a 'Risk Level' (Low/Medium/High) and a 'Remediation Step' for each finding.\n",
        "\"\"\"\n",
        "\n",
        "user_code = \"\"\"\n",
        "def login(username, password):\n",
        "    query = \"SELECT * FROM users WHERE user='\" + username + \"' AND pass='\" + password + \"'\"\n",
        "    execute_sql(query)\n",
        "\"\"\"\n",
        "\n",
        "# The model 'primes' itself to look for SQL Injection, rather than just explaining what the code does.\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": security_auditor_persona},\n",
        "    {\"role\": \"user\", \"content\": f\"Analyze this code: {user_code}\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNOHT5n8js2_",
        "outputId": "a9e50876-95b5-42c5-8ae5-fb014db482c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Code Review Findings\n",
            "\n",
            "#### Finding 1: SQL Injection Vulnerability\n",
            "- **Description**: The code constructs an SQL query by concatenating user inputs (`username` and `password`) directly into the SQL string. This practice is highly prone to SQL injection attacks, where an attacker can manipulate the input to execute arbitrary SQL commands.\n",
            "- **Risk Level**: High\n",
            "- **Remediation Step**: Use parameterized queries (prepared statements) to avoid the direct concatenation of user inputs into SQL queries. For example, if using a library like `sqlite3`, the corrected code would look as follows:\n",
            "    ```python\n",
            "    def login(username, password):\n",
            "        query = \"SELECT * FROM users WHERE user=? AND pass=?\"\n",
            "        execute_sql(query, (username, password))\n",
            "    ```\n",
            "\n",
            "#### Finding 2: Plaintext Password Storage/Comparison\n",
            "- **Description**: The code performs a direct comparison of the `password` input against what is presumably stored in the database. No hashing or salting is mentioned, which is critical for securely storing user passwords.\n",
            "- **Risk Level**: High\n",
            "- **Remediation Step**: Implement a secure password storage strategy. Hash the passwords using a strong hashing function (e.g., bcrypt) when storing them in the database. Likewise, use the same hashing function to compare the given password with the stored hashed password:\n",
            "    ```python\n",
            "    import bcrypt\n",
            "\n",
            "    def login(username, password):\n",
            "        query = \"SELECT pass FROM users WHERE user=?\"\n",
            "        stored_hash = execute_sql(query, (username,)).fetchone()\n",
            "        if stored_hash and bcrypt.checkpw(password.encode('utf-8'), stored_hash[0].encode('utf-8')):\n",
            "            # Successful login\n",
            "        else:\n",
            "            # Failed login\n",
            "    ```\n",
            "\n",
            "### Summary of Findings\n",
            "- The use of string concatenation in SQL queries creates a significant risk for SQL injection, and direct password comparisons without hashing compromise user password security. Implementing prepared statements and secure password hashing methods should be prioritized to mitigate these vulnerabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Contextual Prompting\n",
        "**What it is**\n",
        "*   Providing background information before the task.\n",
        "\n",
        "**What you MUST learn**\n",
        "*   What context is useful vs noise\n",
        "*   Ordering of context (before instructions)\n",
        "*   Summarising long context effectively\n",
        "\n",
        "**Key skills**\n",
        "*   Context compression\n",
        "*  Information relevance filtering\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250704120438594147/Contextual-prompting.webp\" />\n",
        "\n",
        "\n",
        "**Contextual Prompting** is a technique where you provide the AI with relevant background information, specific instructions, tone, and objectives within the prompt. This ensures that the generated responses are tailored to the user's specific needs rather than being generic.\n",
        "\n",
        "### Workflow for Effective Contextual Prompting\n",
        "\n",
        "1. **Define the Context/Background:** Clearly state the subject, scenario, or historical events the AI should address.\n",
        "2. **Specify Tone or Style:** Indicate how the response should sound (e.g., formal, academic, conversational, or simple).\n",
        "3. **Include Details or Constraints:** Add specific focus areas or limitations (e.g., \"Focus on economic factors,\" \"Do not exceed 100 words\").\n",
        "4. **State the Objective:** Make the goal clear (e.g., \"The goal is to explain photosynthesis to school children\").\n",
        "\n",
        "### Importance of Context\n",
        "\n",
        "* **Improves Relevance:** Tailors output to the specific situation, avoiding generic filler.\n",
        "* **Reduces Ambiguity:** Helps the AI avoid misunderstandings by clarifying the focus.\n",
        "* **Enables Customization:** Adjusts depth and complexity based on the target audience.\n",
        "\n",
        "### Best Practices & Tips\n",
        "\n",
        "* **Be Specific and Concise:** Provide enough detail to be clear, but avoid \"noise\" that might confuse the model.\n",
        "* **Iterate:** Review the output and refine the context if the result isn't perfect.\n",
        "* **Use Real Examples:** Providing a sample of the desired data or output style within the context significantly improves accuracy.\n",
        "\n",
        "### Examples of Contextual Prompting\n",
        "\n",
        "#### Example 1: Educational Focus\n",
        "\n",
        "> \"Explain the process of photosynthesis using a **simple and educational tone** suitable for school children. Focus on the **role of sunlight and chlorophyll**. The goal is to help students understand how plants make food.\"\n",
        "\n",
        "#### Example 2: Customer Support Role\n",
        "\n",
        "> \"**Context:** You are a customer support agent responding to a late/damaged order complaint. **Tone:** Helpful and empathetic. **Goal:** Reassure the customer and offer a specific solution.\"\n",
        "\n",
        "### Challenges & Limitations\n",
        "\n",
        "* **Information Overload:** Providing too much irrelevant context (noise) can lead to the model ignoring key details.\n",
        "* **Context Window Limits:** There is a physical limit to how much background information can be processed in a single prompt.\n",
        "* **Accuracy:** Even with context, the AI might hallucinate if the provided information is contradictory or vague."
      ],
      "metadata": {
        "id": "pUvWvH6in2lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. THE CONTEXT (The 'Background' info the model doesn't know)\n",
        "raw_context = \"\"\"\n",
        "Product: X-Phone 15.\n",
        "Released: Nov 2024.\n",
        "Known Issues: Overheating during gaming, slow wireless charging on third-party pads.\n",
        "Support Policy: 1-year limited warranty. Returns accepted within 30 days.\n",
        "\"\"\"\n",
        "\n",
        "# 2. THE TASK (Instruction)\n",
        "# Note the order: Context first, then the specific Task\n",
        "def get_support_response(user_question):\n",
        "    prompt = f\"\"\"\n",
        "    ### BACKGROUND CONTEXT ###\n",
        "    {raw_context}\n",
        "\n",
        "    ### INSTRUCTION ###\n",
        "    You are a customer support bot. Using ONLY the background context above,\n",
        "    answer the user's question. If the information is not in the context,\n",
        "    say \"I am sorry, I do not have information on that specific topic.\"\n",
        "\n",
        "    ### USER QUESTION ###\n",
        "    {user_question}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "print(get_support_response(\"Does the X-Phone 15 have a titanium frame?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W57K3XOUoGjv",
        "outputId": "d573b537-aa6b-4ce7-dcce-4b6a09398676"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry, I do not have information on that specific topic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Chain-of-Thought (CoT) Prompting\n",
        "**What it is**\n",
        "\n",
        "*   Encouraging the model to reason step-by-step.\n",
        "\n",
        "**What you MUST learn**\n",
        "\n",
        "*   When to request reasoning explicitly\n",
        "*   When NOT to expose reasoning (production systems)\n",
        "*   CoT vs short-answer trade-off\n",
        "\n",
        "**Key skills**\n",
        "\n",
        "*   Logical decomposition\n",
        "*   Multi-step reasoning\n",
        "\n",
        "**Chain-of-Thought (CoT)** prompting is a technique that guides AI models to explain their reasoning step-by-step instead of providing a direct answer. By breaking a problem into smaller logical steps, CoT improves accuracy, clarity, and reliability.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Reasoning Guide:** A method that encourages the model to \"think out loud\" before arriving at a conclusion.\n",
        "* **Problem Decomposition:** Dividing complex queries into manageable intermediate steps.\n",
        "* **Applicability:** Highly effective for math problems, commonsense reasoning, and logical puzzles.\n",
        "\n",
        "### How it Works\n",
        "\n",
        "1. **Problem Understanding:** The model receives the initial query or puzzle.\n",
        "2. **Intermediate Reasoning:** Instead of a direct jump, the model creates a \"chain\" of logic where each step leads to the next.\n",
        "3. **Final Answer:** The conclusion is derived only after the intermediate steps are explicitly outlined.\n",
        "4. **Feedback Loop:** An iterative process where reasoning can be refined if an error occurs.\n",
        "\n",
        "### Key Benefits\n",
        "\n",
        "* **Improved Problem Solving:** Enhances the model's ability to handle multi-step challenges (e.g., \"If John > Sarah and Sarah > Tom, who is shortest?\").\n",
        "* **Transparency & Interpretability:** Moves the AI away from being a \"black box\" by making its internal logic visible to the user.\n",
        "* **Enhanced Accuracy:** Minimizes the risk of skipping critical logic or making arithmetic \"leaps\" that lead to errors.\n",
        "* **Complex Task Management:** Excels in story generation, sentiment analysis with nuances, and scientific reasoning.\n",
        "\n",
        "### Challenges\n",
        "\n",
        "* **Computational Cost:** Generating multiple intermediate tokens is more time-consuming and computationally expensive than direct answers.\n",
        "* **Data Dependency:** Effective CoT often requires specific training or high-quality few-shot examples that demonstrate reasoning.\n",
        "* **Coherence Risks:** In very long chains (many steps), the model may struggle to maintain consistent context or logic until the end.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison: Standard vs. CoT\n",
        "\n",
        "| Feature | Standard Prompting | Chain-of-Thought |\n",
        "| --- | --- | --- |\n",
        "| **Directness** | Provides answer immediately | Lists reasoning steps first |\n",
        "| **Transparency** | \"Black box\" (hidden logic) | Open reasoning (traceable) |\n",
        "| **Logic Handling** | Prone to arithmetic/logic errors | Higher accuracy on multi-step tasks |\n",
        "| **Output Length** | Short and concise | Longer and detailed |\n",
        "\n",
        "---\n",
        "\n",
        "### Application Examples\n",
        "\n",
        "#### 1. Mathematical Reasoning\n",
        "\n",
        "* **Standard:** \"What is 39 * 21?\" -> \"819\"\n",
        "* **CoT:** \"First, multiply 30 by 21 to get 630. Then, multiply 9 by 21 to get 189. Finally, add 630 + 189 = 819.\"\n",
        "\n",
        "#### 2. Commonsense Reasoning\n",
        "\n",
        "* **Task:** Identify the shortest person among John, Sarah, and Tom given their height relationships.\n",
        "* **CoT Path:** \"John is taller than Sarah. Sarah is taller than Tom. This forms a sequence: John > Sarah > Tom. Therefore, Tom is the shortest.\"\n",
        "\n",
        "### Best Practices (GeeksforGeeks Tips)\n",
        "\n",
        "* **Iterative Refinement:** If the model gets a logic step wrong, re-prompt it by pointing out the specific step that failed.\n",
        "* **Step Labels:** Use numbered lists or clear markers (e.g., \"Step 1:\", \"Therefore:\") to keep the chain organized.\n",
        "* **Model Size:** CoT is most effective on larger models; smaller models may occasionally generate illogical \"reasoning\" that leads to the wrong answer anyway."
      ],
      "metadata": {
        "id": "jwrEYtV7rQaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_complex_problem(question):\n",
        "    # We ask the model to reason first, then provide the answer in a specific format\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "\n",
        "    Instruction: First, provide a step-by-step logical derivation of the answer.\n",
        "    Then, provide the final answer clearly labeled as 'FINAL_ANSWER'.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    full_text = response.choices[0].message.content\n",
        "\n",
        "    # In a production system, you might split the response to hide the 'thought'\n",
        "    if \"FINAL_ANSWER\" in full_text:\n",
        "        thought_process, final_answer = full_text.split(\"FINAL_ANSWER\")\n",
        "        return final_answer.strip()\n",
        "    return full_text\n",
        "\n",
        "prompt = \"\"\"\n",
        "Question: A farmer has 15 sheep. All but 8 die. How many sheep are left?\n",
        "\n",
        "Instruction: Let's think step by step:\n",
        "1. Identify the total number of sheep started with.\n",
        "2. Carefully interpret the phrase 'all but 8 die'.\n",
        "3. Determine how many survived based on that interpretation.\n",
        "4. State the final count.\n",
        "\"\"\"\n",
        "\n",
        "response =  client.chat.completions.create(\n",
        "  model=\"gpt-4\",\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP3V0bKRrjGs",
        "outputId": "6a4c37f5-9462-4d51-a4bc-1f1c5c07a153"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The farmer starts with 15 sheep.\n",
            "2. The phrase 'all but 8 die' means that 8 sheep did not die.\n",
            "3. Therefore, 8 sheep survive based on this interpretation. \n",
            "4. Thus, the final count is 8 sheep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Step-by-Step Prompting\n",
        "\n",
        "**Step-by-Step Prompting** is a structural technique where you decompose a complex request into a sequence of **ordered, executable instructions**. While it looks similar to Chain-of-Thought (CoT), its primary goal is **procedural execution** rather than logical justification.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Procedural Blueprint:** It provides a \"recipe\" for the AI to follow.\n",
        "* **Execution Focus:** Instead of asking the model to *explain* its reasoning, you are telling the model exactly *how* to process the data in stages.\n",
        "* **Lighter than CoT:** It avoids the \"thought bloat\" of CoT by focusing on the action taken at each step rather than the \"why\" behind it.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Breaking Tasks into Ordered Steps:** Identifying the natural start, middle, and end of a workflow.\n",
        "* **Using Numbered Instructions:** Using a `1., 2., 3.` format, which helps the model track its progress through the \"context window.\"\n",
        "* **Ensuring Step Dependency:** Making sure Step 2 uses the specific output created by Step 1 (e.g., \"Find the name, *then* capitalize it\").\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Procedural Prompting:** Writing instructions as a series of commands.\n",
        "* **Workflow Modeling:** Mapping out a business or technical process (like a CI/CD pipeline or a customer refund) into a prompt.\n",
        "\n",
        "\n",
        "### Difference: CoT vs. Step-by-Step Prompting\n",
        "\n",
        "| Feature | Chain-of-Thought (CoT) | Step-by-Step Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Primary Goal** | Better accuracy via **reasoning**. | Consistent output via **structure**. |\n",
        "| **Model Behavior** | \"Thinking out loud\" to solve logic. | Following a checklist to finish a task. |\n",
        "| **Output Style** | Paragraphs of explanation. | Clearly delineated sections or actions. |\n",
        "| **Best Used For** | Math, Logic, Riddles. | Data cleaning, Formatting, Reporting. |\n",
        "\n",
        "\n",
        "### Detailed Explanation & Sample Code\n",
        "\n",
        "In this example, we use Step-by-Step prompting to handle a messy text-processing task. Note how each step depends on the one before it.\n",
        "\n",
        "#### The Workflow Logic:\n",
        "\n",
        "1. **Filter:** Remove non-essential text.\n",
        "2. **Transform:** Change the format of the remaining data.\n",
        "3. **Audit:** Ensure the output meets a specific rule.\n",
        "\n",
        "### Why this works:\n",
        "\n",
        "1. **Procedural Prompting:** By using \"Step 1, Step 2,\" we prevent the model from getting overwhelmed by the messy input text.\n",
        "2. **Step Dependency:** Step 4 cannot happen correctly unless Step 1 and 2 were successful.\n",
        "3. **No Reasoning Bloat:** Unlike CoT, the model won't say \"I am looking at Alice because she is a user...\" It simply identifies Alice and moves to the next command.\n"
      ],
      "metadata": {
        "id": "tx9Y_URrtX-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_data_step_by_step(raw_input):\n",
        "    # This prompt models a specific workflow (Workflow Modeling)\n",
        "    prompt = f\"\"\"\n",
        "    ### INPUT DATA ###\n",
        "    {raw_input}\n",
        "\n",
        "    ### INSTRUCTIONS ###\n",
        "    Follow these steps exactly:\n",
        "    Step 1: Identify all names mentioned in the text.\n",
        "    Step 2: Convert those names to UPPERCASE.\n",
        "    Step 3: Extract the associated 'Status' for each name.\n",
        "    Step 4: Format the final result as a Python Dictionary where keys are names and values are statuses.\n",
        "\n",
        "    ### OUTPUT ###\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Input with 'noise'\n",
        "messy_text = \"Internal Log: User alice (Status: active) logged in. Later, user bob (Status: suspended) tried to access.\"\n",
        "\n",
        "print(process_data_step_by_step(messy_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNslsWFAtXEk",
        "outputId": "ad3bdbdb-ab72-43e3-e579-97a3f0eff4fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ALICE': 'active', 'BOB': 'suspended'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Self-Consistency Prompting\n",
        "\n",
        "**Self-consistency prompting** is an advanced reliability technique where a model is prompted to generate **multiple independent reasoning paths** for the same problem. The final answer is chosen by identifying the most common (consistent) result among all those paths.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Consensus-Based AI:** Instead of trusting a single \"train of thought,\" the system looks for the \"majority vote.\"\n",
        "* **Advanced CoT:** It builds on Chain-of-Thought (CoT) by adding a layer of validation through repetition.\n",
        "* **Error Mitigation:** It assumes that while a model might make a mistake in one reasoning path, it is unlikely to make the *same* mistake across 5 or 10 different paths.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Sampling Strategies:** Understanding how to set `temperature > 0` so the model generates diverse reasoning paths instead of the same one repeatedly.\n",
        "* **Consensus Selection:** Applying \"Majority Voting\" or \"Aggregation\" logic to find the winning answer.\n",
        "* **Error Reduction Techniques:** Identifying \"outlier\" answers that should be discarded (e.g., if 8 paths say \"42\" and 2 paths say \"100\", the \"100\" is likely an error).\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Output Comparison:** Analyzing multiple strings of text to extract a common numerical or categorical answer.\n",
        "* **Robust Answer Selection:** Designing systems that can handle \"ties\" or \"conflicting responses\" without crashing.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison: Self-Consistency vs. CoT\n",
        "\n",
        "| Feature | Chain-of-Thought (CoT) | Self-Consistency |\n",
        "| --- | --- | --- |\n",
        "| **Execution** | Runs once. | Runs N times (e.g., 5 or 10). |\n",
        "| **Reliability** | Medium (vulnerable to one logic slip). | High (robust against single-point failure). |\n",
        "| **Logic** | Follows a single linear path. | Explores multiple different \"angles\". |\n",
        "| **Cost** | 1x Token Cost. | Nx Token Cost (More expensive). |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices (from GeeksforGeeks)\n",
        "\n",
        "1. **Use for Quantifiable Tasks:** Works best when the final answer is a number, a date, or a \"Yes/No\" choice (easy to count votes).\n",
        "2. **High Temperature is Key:** If `temperature=0`, the model will give you the exact same reasoning path every time, making self-consistency useless.\n",
        "3. **Aggregation Logic:** For complex text answers, you might need a second \"Judge LLM\" to compare the outputs and decide which one is the most consistent.\n",
        "\n",
        "**Would you like me to move on to \"Tree-of-Thought\" (ToT) prompting, which is the most advanced version of this reasoning family?**\n",
        "\n"
      ],
      "metadata": {
        "id": "orp12qu_taz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Extracts the last numerical value from a string.\"\"\"\n",
        "    pattern = r\"-?\\d+\\.?\\d*\"\n",
        "    matches = re.findall(pattern, text)\n",
        "    if matches:\n",
        "        last_match = matches[-1]\n",
        "        return float(last_match) if \".\" in last_match else int(last_match)\n",
        "    return None\n",
        "\n",
        "def get_consistent_answer(question):\n",
        "    responses = []\n",
        "\n",
        "    # We run the prompt 3 times to find a consensus\n",
        "    for i in range(3):\n",
        "        prompt = f\"Question: {question}\\nInstruction: Think step-by-step and end with 'The answer is: [number]'.\"\n",
        "\n",
        "        # Temperature > 0 is REQUIRED for self-consistency to get different paths\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content\n",
        "        print(f\"{i} {content}\")\n",
        "        answer = extract_number(content)\n",
        "\n",
        "        if answer is not None:\n",
        "            responses.append(answer)\n",
        "\n",
        "    if not responses:\n",
        "        return \"No numerical answer could be determined.\"\n",
        "\n",
        "    # Consensus Selection (Majority Voting)\n",
        "    vote_count = Counter(responses)\n",
        "    # Get the most common answer\n",
        "    final_answer, count = vote_count.most_common(1)[0]\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "# Example Usage\n",
        "riddle = \"A bat and a ball cost $1.10. The bat costs $1.00 more than the ball. How much is the ball?\"\n",
        "print(f\"Consistent Answer: {get_consistent_answer(riddle)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBQ9_EECTmnS",
        "outputId": "2a9db3b3-3740-49a6-b4be-fb0ae0ff792e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Step 1: Let's denote the cost of the ball as x (in dollars). \n",
            "\n",
            "Step 2: According to the problem, the bat costs $1.00 more than the ball. Therefore, the bat costs x + $1.00. \n",
            "\n",
            "Step 3: The total cost of the bat and the ball is $1.10. Therefore, we can write the equation: x (cost of the ball) + x + $1.00 (cost of the bat) = $1.10.\n",
            "\n",
            "Step 4: Combining like terms gives 2x + $1.00 = $1.10.\n",
            "\n",
            "Step 5: Subtract $1.00 from both sides of the equation to isolate the term with x. This gives 2x = $0.10.\n",
            "\n",
            "Step 6: Divide both sides of the equation by 2 to solve for x. This gives x = $0.05.\n",
            "\n",
            "The answer is: $0.05.\n",
            "1 First, let's define the problem with equations. Let's say B is the cost of the bat and b is the cost of the ball.\n",
            "\n",
            "From the problem, we know that:\n",
            "\n",
            "B = b + $1.00 (the bat costs $1.00 more than the ball)\n",
            "B + b = $1.10 (the bat and the ball together cost $1.10)\n",
            "\n",
            "We can substitute the first equation into the second to get:\n",
            "\n",
            "(b + $1.00) + b = $1.10\n",
            "2b + $1.00 = $1.10\n",
            "2b = $1.10 - $1.00\n",
            "2b = $0.10\n",
            "b = $0.10 / 2\n",
            "b = $0.05\n",
            "\n",
            "So, the ball costs $0.05.\n",
            "\n",
            "The answer is: $0.05.\n",
            "2 Step 1: We know the total cost of the bat and the ball is $1.10.\n",
            "Step 2: We also know the bat costs $1.00 more than the ball.\n",
            "Step 3: If we assume that the cost of the ball is x, then the cost of the bat would be x + $1.00.\n",
            "Step 4: Using the equation x + (x + $1.00) = $1.10, where the left side represents the total cost of the bat and the ball.\n",
            "Step 5: Solving the equation gives x = $0.05. Therefore, the ball costs $0.05 and the bat costs $1.05.\n",
            "\n",
            "The answer is: $0.05.\n",
            "Consistent Answer: 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. ReAct (Reasoning + Acting) Prompting\n",
        "\n",
        "ReAct is an advanced prompting framework that combines **Chain-of-Thought (CoT) reasoning** with **Action execution**. It allows an AI model to interact with external tools (like search engines, calculators, or databases) in an iterative loop to solve complex, real-world tasks.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Dynamic Interactivity:** Unlike static prompts, ReAct allows the model to \"step out\" of its internal knowledge to gather fresh data.\n",
        "* **The Synergistic Loop:** Reasoning helps the model plan and track actions, while the results of those actions (observations) help the model refine its reasoning.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **The \"Thought → Action → Observation\" Loop:** * **Thought:** The model describes what it needs to do and why.\n",
        "* **Action:** The model calls a specific tool (e.g., `Search[query]`).\n",
        "* **Observation:** The system feeds the tool's result back into the prompt.\n",
        "\n",
        "\n",
        "* **Tool Calling Structure:** How to format instructions so the model outputs commands that your code can actually parse (e.g., JSON or specific string patterns).\n",
        "* **Error Recovery Logic:** Teaching the model what to do if a tool returns an error or no results (e.g., \"The search found nothing, I will try a different keyword\").\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Agent-style Prompting:** Moving from \"answering questions\" to \"managing a process.\"\n",
        "* **Tool Orchestration:** Managing the flow of data between the LLM and the external environment.\n",
        "\n",
        "\n",
        "### Comparison: CoT vs. ReAct\n",
        "\n",
        "| Feature | Chain-of-Thought (CoT) | ReAct Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Information Source** | Internal knowledge only. | Internal knowledge + External tools. |\n",
        "| **Logic Type** | Linear reasoning. | Closed-loop (Reasoning ↔ Action). |\n",
        "| **Best For** | Math, Riddles, Logic. | Research, Fact-checking, Live Data. |\n",
        "| **Outcome** | Static text. | Dynamic interaction with the world. |\n",
        "\n",
        "\n",
        "\n",
        "### Why this works:\n",
        "\n",
        "1. **Grounding in Reality:** By using the **Observation**, the model doesn't have to guess the weather; it is \"grounded\" by the tool's data.\n",
        "2. **Error Recovery:** If `mock_search_tool` returned \"No results,\" the next **Thought** from the model would likely be: *\"The search failed. I will try searching for Tokyo's climate instead.\"*\n",
        "3. **Transparency:** You can see exactly why the model chose to use a tool, making the AI's behavior much easier to audit.\n",
        "\n"
      ],
      "metadata": {
        "id": "ABwFcW4j5w8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import create_agent"
      ],
      "metadata": {
        "id": "aTq_vlB16FHm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(api_key=open_api_key,model=\"gpt-4o-mini\" ,temperature=0)\n",
        "\n",
        "tools = load_tools([\"google-serper\",\"llm-math\"],llm=llm)\n",
        "agent = create_agent(\n",
        "    llm,\n",
        "    tools\n",
        ")\n",
        "response = agent.invoke({\n",
        "    \"messages\": [(\"user\", \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")]\n",
        "})\n",
        "print(response[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKvlIdH78GxL",
        "outputId": "a6c972a7-11ad-4de3-b386-fa3b27409575"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olivia Wilde's boyfriend is Caspar Jopling, who is currently 33 years old. His age raised to the 0.23 power is approximately 2.23.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Tree-of-Thought (ToT) Prompting\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250702161831323869/Tree-of-Thoughts.webp\" />\n",
        "\n",
        "**Tree-of-Thought (ToT)** is an advanced prompting framework that generalizes \"Chain-of-Thought\" by allowing the model to explore multiple reasoning paths (branches) simultaneously. It treats problem-solving as a **search through a tree**, where each node is a \"thought\" or an intermediate step toward the solution.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Non-Linear Reasoning:** Unlike CoT, which follows one path, ToT can branch out, evaluate different ideas, and even backtrack if a path looks like a \"dead end.\"\n",
        "* **Heuristic Evaluation:** The model (or an external script) acts as a \"critic\" to score each branch and decide which one is most likely to succeed.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Branch Generation Strategies:** How to prompt the model to propose 3 or 4 different \"next steps\" instead of just one.\n",
        "* **Pruning Low-Quality Paths:** Learning to identify and \"cut\" branches that are logical failures to save time and tokens.\n",
        "* **Computational Cost Awareness:** ToT is extremely \"token-hungry.\" You must balance the need for accuracy against the high cost of generating dozens of potential paths.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Parallel Reasoning:** Mentally or programmatically managing multiple versions of a solution at once.\n",
        "* **Decision Optimization:** Using the model to weigh the pros and cons of its own ideas to select the optimal \"leaf\" (final answer) of the tree.\n",
        "\n",
        "\n",
        "### Comparison: CoT vs. Self-Consistency vs. ToT\n",
        "\n",
        "| Technique | Logic Structure | Best For |\n",
        "| --- | --- | --- |\n",
        "| **Chain-of-Thought** | Linear (A \\to B \\to C) | Simple math, clear logic. |\n",
        "| **Self-Consistency** | Multiple Linear Paths | Tasks with a single definite answer (Majority vote). |\n",
        "| **Tree-of-Thought** | Branching & Searching | Creative writing, complex planning, coding architecture. |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices (from GeeksforGeeks)\n",
        "\n",
        "1. **Define the \"Thought\" Unit:** Clearly define what a single \"step\" or \"thought\" should look like (e.g., one line of an equation or one paragraph of a plan).\n",
        "2. **Use Deliberate Search:** Use algorithms like **Breadth-First Search (BFS)** or **Depth-First Search (DFS)** to navigate the thoughts.\n",
        "3. **Backtracking:** If the \"Evaluator\" gives all current branches a low score, prompt the model to return to the previous node and try a completely different direction.\n",
        "\n",
        "### Real-World Use Case: The \"Game of 24\"\n",
        "\n",
        "ToT is famously used to solve the \"Game of 24\" (using four numbers and basic math to reach 24).\n",
        "\n",
        "* **Branch 1:** Try adding the first two numbers. (Score: Low)\n",
        "* **Branch 2:** Try multiplying the first two. (Score: High)\n",
        "* **Action:** Prune Branch 1, continue expanding Branch 2."
      ],
      "metadata": {
        "id": "67_XT4b9V498"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_thoughts(state, n=3):\n",
        "    \"\"\"Proposes 'n' possible next steps based on the current state.\"\"\"\n",
        "    prompt = f\"Given the current plan: '{state}', propose {n} distinct next steps to move forward.\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.8 # Higher temperature for creative branching\n",
        "    )\n",
        "    # Assume the model returns a numbered list; we split it into a list\n",
        "    return response.choices[0].message.content.split('\\n')\n",
        "\n",
        "def evaluate_thoughts(thoughts):\n",
        "    \"\"\"Acts as the 'Evaluator' to score each branch.\"\"\"\n",
        "    eval_prompt = f\"Evaluate the following ideas for feasibility on a scale of 1-10: {thoughts}. Respond with only the scores separated by commas.\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    # Convert \"8, 4, 9\" into [8, 4, 9]\n",
        "    return [int(s.strip()) for s in response.choices[0].message.content.split(',')]\n",
        "\n",
        "def tot_controller(initial_problem):\n",
        "    print(f\"Starting Tree-of-Thought for: {initial_problem}\\n\")\n",
        "\n",
        "    # Step 1: Branching (Propose)\n",
        "    branches = generate_thoughts(initial_problem, n=3)\n",
        "\n",
        "    # Step 2: Evaluation (Pruning)\n",
        "    scores = evaluate_thoughts(branches)\n",
        "\n",
        "    # Step 3: Selection\n",
        "    # Combine thoughts with scores and find the best one\n",
        "    thought_scores = list(zip(branches, scores))\n",
        "    best_thought = max(thought_scores, key=lambda x: x[1])\n",
        "\n",
        "    print(f\"Evaluated Branches: {thought_scores}\")\n",
        "    print(f\"\\nSelected Best Path: {best_thought[0]}\")\n",
        "\n",
        "    # Step 4: Final Expansion\n",
        "    # You would typically repeat this loop, but here we generate the final result\n",
        "    final_prompt = f\"Based on the best path '{best_thought[0]}', provide the complete solution.\"\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}]\n",
        "    )\n",
        "    return final_response.choices[0].message.content\n",
        "\n",
        "# Example Execution\n",
        "problem = \"Design a marketing strategy for a new eco-friendly water bottle.\"\n",
        "print(tot_controller(problem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99VIMdo8WTSf",
        "outputId": "1448f0bf-31fd-445c-fdce-af4c0b0352d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Tree-of-Thought for: Design a marketing strategy for a new eco-friendly water bottle.\n",
            "\n",
            "Evaluated Branches: [('1. Perform Market Research: Conduct a comprehensive study on the current market trends. Understand who your target audience is and what they value in an eco-friendly water bottle. Learn about competitors and their strategies, strengths, and weaknesses. ', 10), ('', 10), ('2. Product Positioning and Branding: Based on the market research, determine how to position the product in the market. Define the unique selling points of the water bottle and build a brand image that emphasizes its eco-friendliness and other key features.', 10)]\n",
            "\n",
            "Selected Best Path: 1. Perform Market Research: Conduct a comprehensive study on the current market trends. Understand who your target audience is and what they value in an eco-friendly water bottle. Learn about competitors and their strategies, strengths, and weaknesses. \n",
            "1. Perform Market Research\n",
            "\n",
            "The first step requires a comprehensive research approach that will explore current market trends associated with eco-friendly water bottles. This will involve using resources such as market reports, consumer behavior studies, and trend forecasting services. It’s essential to keep abreast of the changing market dynamics, consumer preferences, and latest innovations in the sector.\n",
            "\n",
            "2. Understand Your Target Audience\n",
            "\n",
            "Identifying your target audience is crucial. You should be aware of their needs, values, habits, likes, and dislikes. For eco-friendly water bottles, the target audience would likely be environmentally conscious individuals who value sustainability, health, and convenience. It may be beneficial to create buyer personas: archetypal representations of your ideal customers. These should be detailed, including demographic information, interests, and buying behavior.\n",
            "\n",
            "3. Analyze Competitors\n",
            "\n",
            "Understanding the competitive landscape is crucial to your product’s success. Identify your main competitors and study their products, strategies, strengths, and weaknesses. Look at their marketing strategies, market share, customer reviews, and public perception. Using platforms like Google, Amazon, and social media channels, can provide insights into competitors’ performance and customer feedback. Moreover, utilizing tools like SWOT analysis (Strengths, Weaknesses, Opportunities, Threats) can provide a comprehensive picture of how your offering compares. \n",
            "\n",
            "4. Evaluate Their Strategies\n",
            "\n",
            "Further to studying your competition, closely inspect their marketing, pricing, and sales strategies. Identify the tactics that have been successful for them, and those that haven't. Understanding why certain strategies have worked for your competitors can provide a blueprint for what may work in your own business.\n",
            "\n",
            "5. Understand Their Strengths and Weaknesses\n",
            "\n",
            "By understanding your competitors' strengths and weaknesses, you can leverage this analysis to your own advantage. You might adapt your own strategies to exploit their weaknesses or emulate their strengths. \n",
            "\n",
            "6. Product Development\n",
            "\n",
            "Using the information that you've gathered so far, you can now start developing your eco-friendly water bottle. Ensure it meets the needs and preferences of your target audience while positioning it uniquely in the market to give you a competitive edge. \n",
            "\n",
            "7. Launch Marketing Campaign \n",
            "\n",
            "Market your product effectively to reach your target audience. Use the marketing strategies you’ve determined to be most successful among your competitors while incorporating unique selling points of your product. This could include social media marketing, influencer partnerships, PR releases, and even attending trade shows or eco-conscious events.\n",
            "\n",
            "8. Measure and Update\n",
            "\n",
            "After launching your product, continuously monitor its performance and feedback from your customers. Use this information to tweak and optimize your product and marketing strategies for better results. \n",
            "\n",
            "Through these steps, the process of launching an eco-friendly water bottle will be guided by in-depth research and strategic planning, hence more likely to be successful in the competitive market.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Deliberate Prompting\n",
        "\n",
        "**Deliberate Prompting** is a technique used to overcome the \"fast-thinking\" (System 1) nature of LLMs by forcing them into a \"slow-thinking\" (System 2) mode. Instead of letting the model predict the next token as quickly as possible, you embed structural pauses and self-correction steps that mirror human deliberation.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Inhibiting Impulse:** Preventing the model from giving the most \"probable\" (but potentially wrong) answer immediately.\n",
        "* **Metacognition:** Prompting the model to think about its own thinking process.\n",
        "* **Reasoning Models (o1/o3 style):** While newer models like OpenAI o1 do this automatically \"under the hood,\" Deliberate Prompting is the manual method for getting standard models (GPT-4o, Claude 3.5) to achieve similar deep reasoning.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Reflection Triggers:** Specific keywords or instructions that halt the model's generation to check for errors (e.g., *\"Wait. Before answering, review your logic for any hidden assumptions\"*).\n",
        "* **Multi-pass Thinking:** Designing a workflow where the first output is treated only as a \"rough draft\" that must pass through a secondary \"critique\" phase.\n",
        "* **Quality vs. Latency Trade-off:** Deep reasoning takes significantly more time and tokens. You must learn to reserve Deliberate Prompting for high-stakes tasks (coding, legal, medical) where accuracy is more valuable than speed.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Deep Reasoning Control:** Orchestrating specific logical checkpoints the model must pass (e.g., Fact-check -> Logical proof -> Final Summary).\n",
        "* **Answer Refinement:** The art of \"iterative prompting\" where the output is polished through successive layers of self-critique.\n",
        "\n",
        "\n",
        "### Comparison: CoT vs. Deliberate Prompting\n",
        "\n",
        "| Feature | Chain-of-Thought (CoT) | Deliberate Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Philosophy** | \"Show your work.\" | \"Think before you speak (and check again).\" |\n",
        "| **Structure** | Single linear sequence. | Multi-pass (Draft → Critique → Refine). |\n",
        "| **Self-Correction** | Accidental (may happen). | **Mandatory** (forced by prompt). |\n",
        "| **Latency** | Medium. | High (requires multiple API calls). |\n",
        "\n",
        "### Summary Checklist for Deliberate Design\n",
        "\n",
        "1. **Stop Phrases:** Use \"Wait,\" \"Evaluate,\" or \"Hold on\" to reset the model's focus.\n",
        "2. **Separate Personas:** Sometimes it helps to tell the model in Pass 2 to \"Act as a harsh editor who hates errors.\"\n",
        "3. **Checkpoints:** If the task is long, ask the model to provide a \"Confidence Score\" (1-10) for its own answer; if it's below 8, force a rewrite."
      ],
      "metadata": {
        "id": "N8ZS4iaST0Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def deliberate_reasoning_flow(complex_question):\n",
        "    # PASS 1: The Draft (System 1 Thinking)\n",
        "    draft_prompt = f\"Solve this: {complex_question}. Just provide a quick initial draft.\"\n",
        "    draft = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": draft_prompt}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # PASS 2: The Reflection (Reflection Trigger)\n",
        "    # This is where we force the 'slow' thinking.\n",
        "    reflection_prompt = f\"\"\"\n",
        "    Here is your initial draft: {draft}\n",
        "\n",
        "    ### INSTRUCTION ###\n",
        "    Analyze this draft critically.\n",
        "    1. Are there any mathematical errors?\n",
        "    2. Did you miss any edge cases?\n",
        "    3. Is there a more efficient way to explain this?\n",
        "    List only the flaws you found.\n",
        "    \"\"\"\n",
        "    critique = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": reflection_prompt}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # PASS 3: The Refinement (Final Answer)\n",
        "    final_prompt = f\"\"\"\n",
        "    Original Question: {complex_question}\n",
        "    Your Draft: {draft}\n",
        "    Your Critique: {critique}\n",
        "\n",
        "    ### TASK ###\n",
        "    Using the critique above, rewrite the solution to be 100% accurate and professional.\n",
        "    \"\"\"\n",
        "    final_answer = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "# Usage\n",
        "problem = \"If a software system needs 99.99% uptime, how many minutes of downtime are allowed per year?\"\n",
        "print(deliberate_reasoning_flow(problem))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujZ9iOd6T38A",
        "outputId": "b73b5876-0d82-4d35-f819-30a8838e8499"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To determine the allowable downtime for a software system that requires 99.99% uptime, we must first calculate the total number of minutes in a year. Let's consider both a common year and a leap year to ensure accuracy.\n",
            "\n",
            "**Common Year:**\n",
            "- 1 year = 365 days\n",
            "- 365 days × 24 hours/day = 8,760 hours\n",
            "- 8,760 hours × 60 minutes/hour = 525,600 minutes\n",
            "\n",
            "For 99.99% uptime, the system can afford to be down for 0.01% of the total time in a common year:\n",
            "- 0.01% of 525,600 minutes = 0.0001 × 525,600 minutes ≈ 52.56 minutes\n",
            "\n",
            "**Leap Year:**\n",
            "- 1 year = 366 days\n",
            "- 366 days × 24 hours/day = 8,784 hours\n",
            "- 8,784 hours × 60 minutes/hour = 527,040 minutes\n",
            "\n",
            "For 99.99% uptime in a leap year:\n",
            "- 0.01% of 527,040 minutes = 0.0001 × 527,040 minutes ≈ 52.704 minutes\n",
            "\n",
            "In conclusion, for a system requiring 99.99% uptime:\n",
            "- The allowed downtime is approximately 52.56 minutes per year in a common year and approximately 52.704 minutes in a leap year.\n",
            "\n",
            "### Additional Context:\n",
            "99.99% uptime is crucial in scenarios where continuous availability is necessary, such as financial services or critical infrastructures. This standard translates to a very minimal allowable downtime, ensuring high reliability and availability of services.\n",
            "\n",
            "Providing a quick reference can help to understand how different uptime percentages convert to downtime:\n",
            "\n",
            "- **99% uptime**: Approximately 87.6 hours of downtime/year\n",
            "- **99.9% uptime**: Approximately 8.76 hours of downtime/year\n",
            "- **99.99% uptime**: Approximately 52.56 minutes of downtime/year (common year)\n",
            "- **99.999% uptime**: Approximately 5.26 minutes of downtime/year\n",
            "\n",
            "This breakdown assists in understanding and setting realistic targets relative to system availability requirements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Self-Reflection Prompting\n",
        "\n",
        "Self-Reflection (also known as **Self-Critique** or **Reflexion**) is an iterative prompting strategy where a model generates a response, evaluates its own work for errors or bias, and then refines that response to provide a higher-quality final output. It effectively turns the AI into its own \"editor.\"\n",
        "\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **The \"Audit\" Mechanism:** It is a metacognitive process where the model performs an internal audit of its logic, facts, and tone.\n",
        "* **Closing the Loop:** Unlike standard prompts that end after one generation, Self-Reflection creates a feedback loop within the conversation or system.\n",
        "* **System 2 Simulation:** It nudges the model away from \"instinctive\" (System 1) fast-thinking toward a \"methodical\" (System 2) reflective state.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Asking for Evaluation Criteria:** You must explicitly tell the model *what* to look for (e.g., \"Check for logical fallacies,\" \"Verify every date mentioned,\" or \"Analyze for gender bias\").\n",
        "* **Iterative Improvement Loops:** Designing workflows where the model reflects 2–3 times before stopping.\n",
        "* **Bias and Error Detection:** Learning to prompt the model to adopt a \"adversarial\" persona to hunt for its own weaknesses or \"hallucinations.\"\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Meta-reasoning:** The ability to prompt a model to \"think about its thought process.\"\n",
        "* **Quality Assurance (QA):** Building automated pipelines that use a \"Critic LLM\" to score the \"Generator LLM\" to ensure the final result meets production standards.\n",
        "\n",
        "\n",
        "\n",
        "### Comparison: CoT vs. Self-Reflection\n",
        "\n",
        "| Feature | Chain-of-Thought (CoT) | Self-Reflection |\n",
        "| --- | --- | --- |\n",
        "| **Workflow** | Linear steps to a conclusion. | Circular (Generate \\to Critique \\to Revise). |\n",
        "| **Correction** | Relies on logic being right the first time. | Actively searches for and fixes mistakes. |\n",
        "| **Transparency** | Shows \"how\" it got the answer. | Explains \"why\" the first version was flawed. |\n",
        "| **Token Usage** | High. | Very High (Multiple generations). |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Use Different Personas:** Have the model \"generate\" as a Developer but \"reflect\" as a Security Engineer.\n",
        "2. **External Grounding:** In more advanced \"Reflexion\" frameworks, the model is given tool outputs (like a compiler error) to reflect upon.\n",
        "3. **Stop Condition:** Always define a maximum number of reflections (e.g., 3 loops) to prevent infinite loops and wasting tokens.\n"
      ],
      "metadata": {
        "id": "g_7QieYdo1so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_reflective_code_gen(task):\n",
        "    # STEP 1: GENERATE\n",
        "    gen_prompt = f\"Write a Python function to: {task}. Just the code.\"\n",
        "    first_draft = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": gen_prompt}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # STEP 2: REFLECT (Critique for security and efficiency)\n",
        "    critique_prompt = f\"\"\"\n",
        "    Review this Python code for bugs and security vulnerabilities:\n",
        "    {first_draft}\n",
        "\n",
        "    List only the errors or improvements needed.\n",
        "    \"\"\"\n",
        "    critique = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": critique_prompt}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # STEP 3: REFINE (Final result)\n",
        "    refine_prompt = f\"\"\"\n",
        "    Original Task: {task}\n",
        "    First Draft: {first_draft}\n",
        "    Critique: {critique}\n",
        "\n",
        "    Provide the final, secure, and optimized version of the code.\n",
        "    \"\"\"\n",
        "    final_output = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": refine_prompt}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Usage\n",
        "print(self_reflective_code_gen(\"connect to a SQL database and fetch user data\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAtSN6Ffo9ss",
        "outputId": "2940c142-e46f-4ca9-fd10-10c6e8864dfa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Here's a final, secure, and optimized version of the code, incorporating the feedback and best practices previously mentioned:\n",
            "\n",
            "```python\n",
            "import sqlite3\n",
            "import logging\n",
            "\n",
            "# Set up basic logging configuration\n",
            "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
            "\n",
            "def fetch_user_data(database_path, query=\"SELECT * FROM users\", parameters=()):\n",
            "    try:\n",
            "        # Establish a connection and execute the query safely within a context manager\n",
            "        with sqlite3.connect(database_path) as connection:\n",
            "            cursor = connection.cursor()\n",
            "\n",
            "            # Use parameterized queries to prevent SQL injection\n",
            "            cursor.execute(query, parameters)\n",
            "            user_data = cursor.fetchall()\n",
            "\n",
            "            # Log the success of the data retrieval\n",
            "            logging.info(\"Successfully fetched user data.\")\n",
            "            return user_data\n",
            "\n",
            "    except sqlite3.Error as e:\n",
            "        # Log the error instead of printing it\n",
            "        logging.error(f\"An error occurred while fetching user data: {e}\")\n",
            "        return None\n",
            "\n",
            "# Example usage:\n",
            "# Assuming there's a database file named 'example.db' and a table 'users'\n",
            "if __name__ == \"__main__\":\n",
            "    user_data = fetch_user_data('example.db')\n",
            "    if user_data is not None:\n",
            "        for user in user_data:\n",
            "            print(user)\n",
            "```\n",
            "\n",
            "### Key Improvements:\n",
            "\n",
            "1. **Logging**: Incorporated the `logging` module to manage error and info messages, providing a more robust mechanism for dealing with issues. This is better than using print statements, especially for production environments.\n",
            "\n",
            "2. **Context Manager for Connections**: Utilized Python's `with` statement to handle the database connection, which ensures that resources are properly managed and is an efficient way to close the connection automatically.\n",
            "\n",
            "3. **Parameterized Queries**: Even though the example query (\"SELECT * FROM users\") doesn't require parameters, the function includes a `parameters` argument to facilitate safe use of parameterized queries, preventing SQL injection if user input is involved in queries.\n",
            "\n",
            "4. **Robust Error Handling**: Logs detailed error messages to help identify issues without exposing sensitive information.\n",
            "\n",
            "This version of the function should be suitable for basic use cases where you need to fetch data from an SQLite database, while ensuring better safety and resource management. Adjustments may be needed based on specific requirements or when interacting with databases other than SQLite.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Contrastive Prompting\n",
        "\n",
        "**Contrastive Prompting** is a technique where you provide the model with examples of both **successful** and **unsuccessful** outputs. By highlighting the gap between \"good\" and \"bad,\" you define clear boundaries for the model, helping it understand not just what to do, but specifically what to avoid.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Boundary Definition:** Using negative constraints to sharpen the model's understanding of a task.\n",
        "* **Learning from Mistakes:** Showing the model a common error and explaining why it is wrong, which prevents the model from repeating that specific pattern.\n",
        "* **Quality Benchmarking:** Providing a \"Gold Standard\" vs. a \"Sub-par\" example so the model can calibrate its own output quality.\n",
        "\n",
        "### What you MUST learn\n",
        "\n",
        "* **Contrast Framing:** How to structure the prompt so the differences are unmistakable (e.g., using labels like `CORRECT:` and `INCORRECT:`).\n",
        "* **Error Explanation:** Explicitly describing the \"why\" behind a failure. This teaches the model the logic of the error.\n",
        "* **Decision Justification:** Instructing the model to justify its final answer by referencing the positive traits of the good example and the absence of flaws from the bad one.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Comparative Reasoning:** The ability to look at two objects and identify the specific features that make one superior.\n",
        "* **Evaluation Clarity:** Writing concise \"critique notes\" within a prompt that the AI can easily translate into action.\n",
        "\n",
        "\n",
        "### Comparison: Standard Few-Shot vs. Contrastive\n",
        "\n",
        "| Feature | Standard Few-Shot | Contrastive Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Example Type** | Only \"Good\" examples. | \"Good\" + \"Bad\" examples. |\n",
        "| **Model Focus** | Imitation. | Discrimination & Avoidance. |\n",
        "| **Nuance** | Often misses subtle rules. | Highly effective for tone, style, and logic constraints. |\n",
        "| **Best For** | Learning a new format. | Fixing recurring mistakes or refining tone. |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "* **Highlight the \"Why\":** Never just show a bad example; always explain *why* it is bad. The explanation is what actually trains the model's reasoning.\n",
        "* **Symmetry:** Try to use bad examples that are the \"opposite\" of the good ones to show the full spectrum of the task.\n",
        "* **Use for Formatting:** If a model keeps including conversational filler (like \"Sure, I can help with that\"), show an example where that filler is marked as **INCORRECT**.**"
      ],
      "metadata": {
        "id": "dXMexVByqUhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_email_writer(draft_info):\n",
        "    prompt = f\"\"\"\n",
        "    Task: Write a professional email based on these details: {draft_info}\n",
        "\n",
        "    ### EXAMPLES OF WHAT TO DO VS WHAT NOT TO DO ###\n",
        "\n",
        "    BAD EXAMPLE (Too Casual):\n",
        "    \"Hey! Sorry the thing is late. My bad. Will send it soon.\"\n",
        "    Why: Unprofessional, lacks specific details, and uses slang.\n",
        "\n",
        "    BAD EXAMPLE (Too Robotic):\n",
        "    \"Per our previous correspondence, the undersigned apologizes for the temporal delay regarding the deliverables. Your patience is requested.\"\n",
        "    Why: Overly stiff, uses unnecessary jargon, and sounds like a legal document.\n",
        "\n",
        "    GOOD EXAMPLE (Balanced & Professional):\n",
        "    \"I'm writing to apologize for the delay in delivering the report. We encountered a minor technical issue, but it is now resolved. I will have the final version to you by 4 PM today.\"\n",
        "    Why: Clear, takes ownership, provides a specific timeline, and remains polite.\n",
        "\n",
        "    ### FINAL TASK ###\n",
        "    Using the \"Good Example\" as your guide for tone and clarity, write the email for the following:\n",
        "    Details: {draft_info}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Usage\n",
        "print(contrastive_email_writer(\"Apologizing for a missed meeting due to a family emergency.\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tVnSznorY1e",
        "outputId": "909b1f1b-775d-4711-e333-767acc987f18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Apology for Missing Our Scheduled Meeting\n",
            "\n",
            "Dear [Recipient's Name],\n",
            "\n",
            "I hope this message finds you well. I am writing to sincerely apologize for missing our scheduled meeting on [date]. Unfortunately, I was unable to attend due to an unexpected family emergency that required my immediate attention.\n",
            "\n",
            "I understand the importance of our discussion and regret any inconvenience my absence may have caused. Please let me know if it would be possible to reschedule at a time that is convenient for you; I am committed to ensuring we address all pending matters promptly.\n",
            "\n",
            "Thank you for your understanding and support during this unforeseen circumstance. I look forward to our conversation and continuing our collaboration.\n",
            "\n",
            "Warm regards,\n",
            "\n",
            "[Your Name]  \n",
            "[Your Position]  \n",
            "[Your Contact Information]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. Negative Prompting\n",
        "\n",
        "Negative Prompting is the strategy of defining the \"excluded space\" for an AI model. While standard prompts give instructions on what to generate, negative prompts provide a list of boundaries, behaviors, and content types that the model must strictly avoid.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Boundary Setting:** Establishing the \"No-Go\" zones for the model's logic and creativity.\n",
        "* **Noise Reduction:** In image generation (like Midjourney or Stable Diffusion), it removes artifacts like \"extra fingers\" or \"blurry textures.\" In LLMs, it removes \"verbal tics\" or \"corporate jargon.\"\n",
        "* **The \"Inverse\" Instruction:** It is often more efficient to tell a model what *not* to do than to list every single thing it *should* do.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "#### 1. Negative Constraints without Over-restriction\n",
        "\n",
        "Learn the \"Goldilocks\" zone of constraints. If you give too many negatives, the model may become \"constipated\"—either refusing to answer or providing a response so sterile that it loses all value.\n",
        "\n",
        "* **Mistake:** \"Don't use any common words.\" (Impossible task)\n",
        "* **Better:** \"Avoid using clichés or flowery adjectives.\" (Actionable task)\n",
        "\n",
        "#### 2. Combining Positive + Negative Rules\n",
        "\n",
        "The most effective prompts follow a \"Do this, but not that\" structure.\n",
        "\n",
        "* **Example:** \"Write a summary of this meeting (**Positive**). Do not mention the internal budget numbers or the names of the outside vendors (**Negative**).\"\n",
        "\n",
        "#### 3. Avoiding Confusion (The \"Pink Elephant\" Problem)\n",
        "\n",
        "LLMs sometimes struggle with the word \"not\" because their attention mechanism anchors on the noun that follows. If you say \"Do not talk about pink elephants,\" the model's focus is now on \"pink elephants.\"\n",
        "\n",
        "* **Pro-Tip:** Use strong, exclusionary formatting or replace \"Don't do X\" with \"Always do Y instead.\"\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Guardrail Design:** Creating safety barriers that prevent the model from leaking sensitive data, exhibiting bias, or breaking a specific persona.\n",
        "* **Constraint Balancing:** The art of applying enough pressure to keep the model on track without crushing its ability to be helpful or creative.\n",
        "\n",
        "\n",
        "\n",
        "### Comparison: Positive-Only vs. Positive + Negative\n",
        "\n",
        "| Feature | Positive-Only Prompt | Positive + Negative Prompt |\n",
        "| --- | --- | --- |\n",
        "| **Control** | Suggestive; the model might wander. | Rigid; the model stays within bounds. |\n",
        "| **Output Style** | Follows the \"average\" of its training. | Follows a specific, pruned style. |\n",
        "| **Safety** | Relies on the model's built-in filters. | Adds custom layers of protection. |\n",
        "| **Clarity** | Can be ambiguous. | Sharpens intent by defining the opposite. |\n",
        "\n",
        "\n",
        "\n",
        "### Summary Checklist for Negative Design\n",
        "\n",
        "1. **Format clearly:** Use a dedicated `### DO NOT ###` section.\n",
        "2. **Be specific:** \"Don't be rude\" is vague. \"Do not use sarcasm or dismissive language\" is specific.\n",
        "3. **Monitor \"Refusal\":** If the model stops answering, remove one negative constraint at a time to see which one is too restrictive.\n"
      ],
      "metadata": {
        "id": "ZndDS8ByhQRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Code Example: The \"No-Hype\" Marketing Writer\n",
        "\n",
        "\n",
        "def write_product_description(product_name, features):\n",
        "    # The negative prompt is embedded in the system/user instruction\n",
        "    negative_constraints = \"\"\"\n",
        "    ### NEGATIVE CONSTRAINTS (WHAT NOT TO DO) ###\n",
        "    - DO NOT use the following 'AI-typical' words: delve, tap into, revolutionize, unleash, or testament.\n",
        "    - DO NOT use exclamation points.\n",
        "    - DO NOT use more than 3 adjectives per paragraph.\n",
        "    - DO NOT mention the price.\n",
        "    - AVOID corporate jargon; use plain, simple English.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Write a 100-word product description for '{product_name}'.\n",
        "    Features: {features}\n",
        "\n",
        "    {negative_constraints}\n",
        "\n",
        "    Final Version:\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Usage\n",
        "features_list = \"Noise-cancelling, 40-hour battery, vegan leather\"\n",
        "print(write_product_description(\"QuietCloud Headphones\", features_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_TP4abMhTTR",
        "outputId": "6c627460-af5e-40f9-91ec-c495a4a70e0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuietCloud Headphones offer a serene listening experience with their advanced noise-cancelling technology, allowing you to enjoy your favorite tunes without distractions. Designed for comfort, they feature soft vegan leather that gently hugs your ears, making them perfect for long listening sessions. With an impressive 40-hour battery life, you can enjoy uninterrupted sound for extended periods, whether you're commuting, working, or relaxing at home. Their sleek design makes them a stylish accessory for any occasion. QuietCloud Headphones bring together functionality and comfort, providing a peaceful audio experience wherever you go. Discover your sound sanctuary with QuietCloud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Output-Format Prompting\n",
        "\n",
        "**Output-Format Prompting** is a technique where you provide explicit instructions and structural templates to ensure the AI's response is in a specific, machine-readable, or human-organized format. Instead of getting a \"conversational\" response, you get a \"data\" response.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Constraint Enforcement:** It transforms the AI from a chat partner into a structured data generator.\n",
        "* **Eliminating \"Chatter\":** It prevents the model from adding conversational filler like \"Sure, here is your information:\" which can break automated scripts.\n",
        "* **Format Predictability:** Ensuring every response follows the exact same pattern, making it reliable for software to process.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **JSON / Markdown / Table Enforcement:**\n",
        "* **JSON:** The standard for most APIs. You must learn to ask for \"valid, minified JSON\" and provide a list of keys.\n",
        "* **Markdown:** Used for human-readable reports with headers, bold text, and checklists.\n",
        "* **Tables:** Best for comparisons or data summaries within a chat window.\n",
        "\n",
        "\n",
        "* **Schema-driven Prompts:** Learning to use tools like **JSON Schema**, **Pydantic (Python)**, or **Zod (TypeScript)** to define a rigid contract that the model must follow.\n",
        "* **Handling Malformed Outputs:** Sometimes the model misses a closing bracket `}` or a comma. You must learn to implement \"repair\" logic or retries in your code to handle these edge cases.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Structured Generation:** Writing prompts that include \"placeholders\" or \"empty schemas\" for the AI to fill in.\n",
        "* **Downstream System Integration:** Designing prompts so that the output can be directly fed into a database, a UI component, or another API without human manual editing.\n",
        "\n",
        "### Best Practices for Structured Results\n",
        "\n",
        "1. **Stop Word/Tokens:** If your model supports it, use a \"stop token\" like `}` to ensure it doesn't add extra text after the JSON.\n",
        "2. **Few-Shot Formatting:** Provide one example of the *correct* format in the prompt to reduce the chance of the AI \"freestyling.\"\n",
        "3. **Temperature 0:** Always set the `temperature` to 0 or near-zero for formatting tasks. This forces the model to choose the most \"certain\" token, which is usually the one that follows your structure."
      ],
      "metadata": {
        "id": "xTkODaWkbwYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Code Example: Extracting Structured Data\n",
        "\n",
        "def extract_ticket_data(email_text):\n",
        "    # 1. Define the Schema-driven prompt\n",
        "    prompt = f\"\"\"\n",
        "    Extract support ticket details from the email below.\n",
        "\n",
        "    ### OUTPUT FORMAT ###\n",
        "    Respond ONLY with a JSON object following this schema:\n",
        "    {{\n",
        "        \"customer_name\": \"string\",\n",
        "        \"priority\": \"low | medium | high\",\n",
        "        \"issue_category\": \"billing | technical | shipping\",\n",
        "        \"summary\": \"one sentence description\"\n",
        "    }}\n",
        "\n",
        "    ### INPUT EMAIL ###\n",
        "    {email_text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0 # Crucial for keeping formatting strict\n",
        "    )\n",
        "\n",
        "    # 2. Handling Malformed Output (Basic Error Recovery)\n",
        "    try:\n",
        "        raw_output = response.choices[0].message.content\n",
        "        structured_data = json.loads(raw_output)\n",
        "        return structured_data\n",
        "    except json.JSONDecodeError:\n",
        "        return \"Error: Model returned invalid JSON. Retrying or manual check required.\"\n",
        "\n",
        "# Usage\n",
        "email = \"Hi team, I was charged twice for my subscription this morning. Can you fix this? Thanks, Alex.\"\n",
        "ticket = extract_ticket_data(email)\n",
        "print(ticket)\n",
        "# Output: {'customer_name': 'Alex', 'priority': 'high', 'issue_category': 'billing', 'summary': 'User reported a double charge for their subscription.'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54cMNt64s9GC",
        "outputId": "38daf1a5-276d-43d9-8eca-5c126fc80f1b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Model returned invalid JSON. Retrying or manual check required.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. Style Prompting\n",
        "\n",
        "**Style Prompting** is the technique of explicitly directing the \"voice,\" \"tone,\" and \"persona\" of the AI’s output. While standard prompting focuses on **what** the model says, Style Prompting focuses on **how** it says it, ensuring the language is appropriate for the target audience and industry.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Aesthetic Control:** Fine-tuning the vocabulary, sentence structure, and reading level of the response.\n",
        "* **Persona Alignment:** Shifting the model's identity from a \"neutral assistant\" to a \"senior software architect,\" \"supportive tutor,\" or \"playful copywriter.\"\n",
        "* **The \"Vibe\" Filter:** Ensuring the output matches your brand guidelines or a specific professional standard.\n",
        "\n",
        "**Core idea:** Content is the *meat*; style is the *flavor*. Managing style prevents AI from sounding like a generic, robotic chatbot.\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "#### 1. Style Tokens\n",
        "\n",
        "Tokens are specific descriptive keywords that \"steer\" the model's latent space toward certain linguistic patterns.\n",
        "\n",
        "* **Formal/Academic:** Uses passive voice, complex sentence structures, and high-level vocabulary (e.g., \"utilize\" vs \"use\").\n",
        "* **Concise/Direct:** Forces the model to eliminate fluff and use \"Telegram style\" or bullet points.\n",
        "* **Supportive/Empathetic:** Increases the use of validating phrases and active listening markers.\n",
        "\n",
        "#### 2. Domain-Specific Language (DSL) Control\n",
        "\n",
        "Learn to inject industry-specific jargon or terminology correctly without the model making mistakes.\n",
        "\n",
        "* **Example:** For a medical context, you might instruct the model to \"use ICD-10 terminology and avoid colloquialisms.\"\n",
        "\n",
        "#### 3. Style vs. Content Separation\n",
        "\n",
        "This is an engineering discipline where you clearly separate the *Instructions* from the *Style Guide* and the *Data*.\n",
        "\n",
        "* **Delimiters:** Using XML-style tags like `<style_guidelines>` and `<input_data>` to prevent the model from confusing your formatting rules with the facts it needs to process.\n",
        "\n",
        "\n",
        "### Controls & Parameters\n",
        "\n",
        "| Control | Description | Example Instruction |\n",
        "| --- | --- | --- |\n",
        "| **Tone** | The emotional quality of the text. | \"Use a witty and slightly irreverent tone.\" |\n",
        "| **Writing Style** | The structural approach. | \"Write in short, punchy sentences for mobile reading.\" |\n",
        "| **Audience** | The intended reader's expertise. | \"Explain this as if to a Senior VP of Finance.\" |\n",
        "| **Verbosity** | The length and detail level. | \"Extreme brevity; max 20 words per response.\" |\n",
        "\n",
        "\n",
        "\n",
        "### Style Prompting vs. Few-shot Prompting\n",
        "\n",
        "| Aspect | Style Prompting | Few-shot Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Focus** | Tone, voice, and \"flavor.\" | Logic, reasoning, and behavior. |\n",
        "| **Method** | Adjectives and role instructions. | Demonstrations via examples. |\n",
        "| **Data Usage** | Zero-shot or instruction-based. | Requires input-output pairs. |\n",
        "| **Best For** | Brand alignment and audience fit. | Complex formatting or logical tasks. |\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Linguistic Control:** Identifying when a model is defaulting to \"AI-speak\" (e.g., using words like *tapestry* or *delve*) and using negative prompting to strip them out.\n",
        "* **Audience Alignment:** Testing prompts against different \"personas\" to ensure the reading level is calibrated correctly (e.g., Grade 8 reading level vs. Ph.D. level).\n"
      ],
      "metadata": {
        "id": "2HjdOyEtb1gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Code Example: Implementing a Style Layer\n",
        "\n",
        "#In production, you often \"layer\" a style prompt on top of a user's raw query to ensure consistency.\n",
        "\n",
        "def generate_styled_response(user_input, target_audience=\"Junior Dev\"):\n",
        "    # The 'Style Layer' remains consistent regardless of the query\n",
        "    style_guide = {\n",
        "        \"Junior Dev\": \"Use analogies, avoid over-optimization talk, and explain 'why'.\",\n",
        "        \"Senior Dev\": \"Be concise, use technical jargon, and focus on scalability/complexity.\",\n",
        "        \"Product Manager\": \"Focus on value, timelines, and user impact. Skip the code details.\"\n",
        "    }\n",
        "\n",
        "    system_message = f\"\"\"\n",
        "    You are a technical consultant.\n",
        "    ### STYLE RULES ###\n",
        "    Audience: {target_audience}\n",
        "    Approach: {style_guide[target_audience]}\n",
        "    Format: Use Markdown with bolding for key terms.\n",
        "    \"\"\"\n",
        "\n",
        "    # Combined with the user's specific request\n",
        "    prompt = f\"{system_message}\\n\\nUSER REQUEST: {user_input}\"\n",
        "\n",
        "    # response = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": prompt}]...)\n",
        "    return prompt\n",
        "\n",
        "# Output for 'Senior Dev' would look vastly different than 'Junior Dev'\n",
        "print(generate_styled_response(\"Explain Docker\", target_audience=\"Senior Dev\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP-ZzVJphowa",
        "outputId": "3796c4d6-f848-460c-eab1-d5c2eea03303"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    You are a technical consultant.\n",
            "    ### STYLE RULES ###\n",
            "    Audience: Senior Dev\n",
            "    Approach: Be concise, use technical jargon, and focus on scalability/complexity.\n",
            "    Format: Use Markdown with bolding for key terms.\n",
            "    \n",
            "\n",
            "USER REQUEST: Explain Docker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. Length-Controlled Prompting\n",
        "\n",
        "### What is Length-Controlled Prompting?\n",
        "\n",
        "Length-Controlled Prompting is the practice of **explicitly controlling verbosity and output size** of model responses.\n",
        "\n",
        "It is NOT the same as just setting max tokens.\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* Token budgeting\n",
        "* Summarization constraints\n",
        "* Precision vs completeness trade-offs\n",
        "\n",
        "\n",
        "### Why Max Tokens Alone Is Not Enough\n",
        "\n",
        "* Max tokens caps output\n",
        "* Does not control structure\n",
        "* Does not enforce conciseness\n",
        "\n",
        "Length control must be **instructional**, not only parametric.\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* Compression techniques\n",
        "* Output predictability\n",
        "* Structured summarization\n"
      ],
      "metadata": {
        "id": "vvhXxvjGblwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. Prompt Chaining\n",
        "<img src=\"data:image/jpeg;base64,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\" alt=\"Pipeline\" width=\"400\"/>\n",
        "\n",
        "\n",
        "## What It Is\n",
        "\n",
        "**Prompt Chaining** is the practice of breaking a task into multiple connected prompts.\n",
        "\n",
        "- Each prompt handles a specific subtask, and its output becomes the input for the next prompt.\n",
        "- Used to break complex tasks into smaller, manageable steps, enabling more accurate and controlled reasoning.\n",
        "\n",
        "\n",
        "\n",
        "## What You MUST Learn\n",
        "\n",
        "### 1. State Passing Between Prompts\n",
        "\n",
        "- Explicitly pass outputs from one prompt to the next.\n",
        "- State may include:\n",
        "  - Text outputs\n",
        "  - Structured data (JSON)\n",
        "  - Scores, flags, or decisions\n",
        "- ⚠️ **Poor state handling leads to context loss and hallucinations.**\n",
        "\n",
        "### 2. Error Propagation Control\n",
        "\n",
        "- Errors in early prompts can cascade through the chain.\n",
        "- Common control mechanisms:\n",
        "  - Validation prompts\n",
        "  - Output format enforcement\n",
        "  - Confidence checks\n",
        "  - Retry or fallback logic\n",
        "\n",
        "### 3. Modular Prompt Design\n",
        "\n",
        "Each prompt should:\n",
        "- Perform one clear task\n",
        "- Be reusable and replaceable\n",
        "- Have clearly defined inputs and outputs\n",
        "\n",
        "✅ **Encourages scalability and maintainability.**\n",
        "\n",
        "\n",
        "## Key Skills Developed\n",
        "\n",
        "### 🧠 Pipeline Thinking\n",
        "\n",
        "Think in terms of step-by-step data flow:\n",
        "\n",
        "```\n",
        "Input → Process → Transform → Decide → Output\n",
        "```\n",
        "\n",
        "Similar to data engineering and ML pipelines.\n",
        "\n",
        "### ⚙️ Workflow Engineering\n",
        "\n",
        "Designing structured, deterministic AI workflows.\n",
        "\n",
        "Includes:\n",
        "- Sequencing\n",
        "- Branching\n",
        "- Looping\n",
        "- Validation\n",
        "\n",
        "**Foundational for Agentic AI systems.**\n",
        "\n",
        "\n",
        "\n",
        "## How Prompt Chaining Works\n",
        "\n",
        "1. **Initial Prompt**\n",
        "   - Defines the first subtask clearly.\n",
        "\n",
        "2. **Model Output**\n",
        "   - The LLM generates a response based on the prompt.\n",
        "\n",
        "3. **Next Prompt**\n",
        "   - The output is evaluated and embedded into the next prompt.\n",
        "\n",
        "4. **Repeat**\n",
        "   - The process continues until the full task is completed.\n",
        "\n",
        "\n",
        "\n",
        "## Types of Prompt Chaining\n",
        "\n",
        "### 1. Sequential Chaining\n",
        "\n",
        "- Linear execution\n",
        "- Each prompt depends directly on the previous output\n",
        "\n",
        "**Example:** Extract → Summarize → Rewrite\n",
        "\n",
        "### 2. Conditional Chaining\n",
        "\n",
        "- Flow branches based on model output\n",
        "- Different prompts are triggered depending on conditions\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "If sentiment = positive → Generate praise\n",
        "If sentiment = negative → Generate apology\n",
        "```\n",
        "\n",
        "### 3. Looping Chaining\n",
        "\n",
        "- Repeats the same prompt structure over multiple inputs\n",
        "\n",
        "**Example:** Summarizing multiple documents one by one\n",
        "\n",
        "\n",
        "## Why Use Prompt Chaining\n",
        "\n",
        "| Benefit | Description |\n",
        "|---------|-------------|\n",
        "| **Breaks Down Complexity** | Simplifies large tasks into smaller, solvable steps |\n",
        "| **Improves Accuracy** | Guides the model through logical intermediate outputs |\n",
        "| **Enhances Explainability** | Each step is explicit and traceable |\n",
        "| **Maintains Context** | Context is preserved across prompt transitions |\n",
        "| **Manages Context Limits** | Helps work within LLM token constraints by chunking tasks |\n",
        "| **Enables Workflow Automation** | Forms the basis of structured AI pipelines and agents |\n",
        "\n",
        "\n",
        "## Prompt Chaining vs. Chain-of-Thought\n",
        "\n",
        "| Aspect | Prompt Chaining | Chain-of-Thought |\n",
        "|--------|----------------|------------------|\n",
        "| **Structure** | Multiple prompts | Single prompt |\n",
        "| **State** | Explicitly passed | Implicit reasoning |\n",
        "| **Control** | High | Limited |\n",
        "| **Explainability** | External & traceable | Internal to model |\n",
        "| **Best for** | Workflows & agents | Reasoning within one task |\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "- **Prompt Chaining:** Output → Input across multiple prompts\n",
        "- **Chain-of-Thought:** Step-by-step reasoning inside one prompt\n",
        "\n",
        "**Key Takeaway:** Prompt chaining is essential for building robust, scalable AI workflows and is a foundational technique for agentic systems."
      ],
      "metadata": {
        "id": "f5Y9EQYMPdJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0,api_key=open_api_key)\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "extract_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Extract key points from the following text:\\n{text}\"\n",
        ")\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summarize the following key points:\\n{key_points}\"\n",
        ")\n",
        "\n",
        "extract_chain = extract_prompt | llm | parser\n",
        "summary_chain = summary_prompt | llm | parser\n",
        "\n",
        "chain = (\n",
        "    {\"key_points\": extract_chain}\n",
        "    | summary_chain\n",
        ")\n",
        "\n",
        "result = chain.invoke({\n",
        "    \"text\": \"Prompt chaining improves reliability and control in LLM systems.\"\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2S-aUGOaq8M",
        "outputId": "d1e3afb3-0a2d-4b76-931a-01d08e3c8bdd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt chaining improves the reliability of Large Language Model (LLM) systems by allowing for more structured interactions. This technique also offers enhanced control over the outputs produced, leading to more accurate and relevant results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. Retrieval-Augmented Prompting (RAG)\n",
        "\n",
        "Retrieval-Augmented Prompting (commonly known as **RAG**) is the practice of dynamically injecting relevant, external facts into a prompt before sending it to the model. Instead of relying solely on the AI's \"internal memory\" (training data), RAG allows the model to \"look it up\" in a provided library of documents.\n",
        "\n",
        "\n",
        "\n",
        "### What It Is\n",
        "\n",
        "* **Open-Book Exam:** If standard prompting is like an AI taking a test from memory, RAG is like giving the AI the textbook and asking it to find the answer.\n",
        "* **The \"Grounding\" Layer:** It provides a factual foundation for the AI’s response, ensuring it stays tethered to your specific data (PDFs, databases, or live web feeds).\n",
        "* **Just-in-Time Knowledge:** It solves the \"Knowledge Cutoff\" problem by feeding the model current information that wasn't available when it was originally trained.\n",
        "\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "#### 1. Query Formulation\n",
        "\n",
        "Converting a user’s natural question into an optimized \"search term\" for the retriever.\n",
        "\n",
        "* **Technique:** \"HyDE\" (Hypothetical Document Embeddings), where the AI generates a fake \"perfect\" answer first, then uses *that* fake answer to search for real documents that look like it.\n",
        "\n",
        "#### 2. Context Window Management\n",
        "\n",
        "You cannot feed a 1,000-page book into a prompt. You must master:\n",
        "\n",
        "* **Chunking:** Splitting documents into small, logical pieces (e.g., 500-token paragraphs).\n",
        "* **Token Budgeting:** Deciding how much space to give the \"Found Facts\" versus the \"User Question\" so you don't hit the model's memory limit.\n",
        "\n",
        "#### 3. Grounding & Hallucination Reduction\n",
        "\n",
        "The most critical part of RAG prompting is the \"Constraint.\"\n",
        "\n",
        "* **Instruction:** \"Use ONLY the provided context to answer. If the answer isn't there, say you don't know. DO NOT use outside knowledge.\"\n",
        "* **Citations:** Prompting the model to include source tags (e.g., `[Source #1]`) to prove its answer is real.\n",
        "\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Retrieval Integration:** Knowing how to \"stitch\" together a search engine (Vector Database) and the LLM into a single automated pipeline.\n",
        "* **Knowledge Grounding:** Designing \"System Instructions\" that force the model to act as a faithful librarian rather than a creative writer.\n",
        "\n",
        "\n",
        "### Comparison: Standard Prompt vs. RAG Prompt\n",
        "\n",
        "| Feature | Standard Prompt | RAG Prompt |\n",
        "| --- | --- | --- |\n",
        "| **Data Source** | Training data (static). | External docs (dynamic/live). |\n",
        "| **Accuracy** | Prone to \"hallucinations.\" | High (if the retrieved data is correct). |\n",
        "| **Cost** | Lower (shorter prompts). | Higher (retrieved text adds tokens). |\n",
        "| **Use Case** | General chat, creative writing. | Customer support, research, internal tools. |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Reranking:** Don't just trust the first 5 things the search engine finds. Use a \"Reranker\" model to check if those 5 things are *actually* relevant to the question before prompting.\n",
        "2. **Metadata Filtering:** If a user asks \"What happened in 2022?\", filter your search results to only include files with a 2022 timestamp before they reach the prompt.\n",
        "3. **Negative Constraints:** Explicitly tell the model: *\"Avoid saying 'Based on the documents provided...'—just answer the question directly.\"*"
      ],
      "metadata": {
        "id": "rDRiLD_UgOsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "### Code Example: A Simple RAG \"Stitch\"\n",
        "\n",
        "#This is the logic used to combine a retrieved \"Fact\" with a \"Question.\"\n",
        "\n",
        "def generate_rag_prompt(user_query, retrieved_chunks):\n",
        "    # This function creates the 'Augmented' prompt\n",
        "    context_block = \"\\n\".join([f\"DOC {i+1}: {text}\" for i, text in enumerate(retrieved_chunks)])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a Fact-Checking Assistant.\n",
        "    Use the following pieces of retrieved context to answer the user's question.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context_block}\n",
        "\n",
        "    USER QUESTION: {user_query}\n",
        "\n",
        "    RULES:\n",
        "    1. Answer ONLY using the context above.\n",
        "    2. If the answer is not in the context, state: 'Information not found.'\n",
        "    3. Cite the document number for every fact you mention (e.g., [DOC 1]).\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Example retrieval (In a real app, this comes from a Vector DB)\n",
        "chunks = [\"The 2024 company holiday for New Year's is January 1st and 2nd.\",\n",
        "          \"Remote work is allowed up to 3 days per week.\"]\n",
        "query = \"When is the New Year's holiday?\"\n",
        "\n",
        "print(generate_rag_prompt(query, chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwyKj1XMgcZi",
        "outputId": "ec3cca30-2fe0-45b0-c7f8-7a8f5e9b79d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    You are a Fact-Checking Assistant.\n",
            "    Use the following pieces of retrieved context to answer the user's question.\n",
            "\n",
            "    CONTEXT:\n",
            "    DOC 1: The 2024 company holiday for New Year's is January 1st and 2nd.\n",
            "DOC 2: Remote work is allowed up to 3 days per week.\n",
            "\n",
            "    USER QUESTION: When is the New Year's holiday?\n",
            "\n",
            "    RULES:\n",
            "    1. Answer ONLY using the context above.\n",
            "    2. If the answer is not in the context, state: 'Information not found.'\n",
            "    3. Cite the document number for every fact you mention (e.g., [DOC 1]).\n",
            "\n",
            "    ANSWER:\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. Context Compression Prompting\n",
        "\n",
        "**Context Compression Prompting** is the practice of shrinking large amounts of input data (documents, chat histories, or search results) into a smaller, denser format before sending it to a Large Language Model. This allows you to stay within \"Context Window\" limits while significantly reducing token costs and latency.\n",
        "\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **The \"Information Sieve\":** It acts as a filter that removes redundant words, stop-words, or irrelevant sections while preserving the \"semantic core\" of the message.\n",
        "* **Efficiency First:** Instead of feeding a 50-page PDF to an LLM, you compress it into a 2-page highly-concentrated summary or a set of \"key-value\" pairs.\n",
        "* **Cost Management:** Since LLMs charge per token, compression directly lowers the price of every API call.\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "#### 1. Lossy vs. Lossless Compression\n",
        "\n",
        "* **Lossy Compression:** Uses an LLM to summarize text. Some nuance is lost, but the output is highly readable and significantly shorter (e.g., a 1000-word article becomes a 100-word summary).\n",
        "* **Lossless (or Near-Lossless) Compression:** Uses specialized algorithms (like **LLMLingua**) to remove tokens that are mathematically \"redundant\" based on probability. The result might look like gibberish to humans but remains perfectly understandable to the AI.\n",
        "\n",
        "#### 2. Relevance Scoring\n",
        "\n",
        "You must learn how to rank chunks of text based on their importance to the current user query.\n",
        "\n",
        "* **Concept:** If the user asks about \"Revenue,\" a paragraph about \"Employee Benefits\" should receive a low relevance score and be \"compressed\" out of existence for that specific turn.\n",
        "\n",
        "#### 3. Chunking Strategies\n",
        "\n",
        "Before you can compress, you must break text down.\n",
        "\n",
        "* **Fixed-size:** Breaking at exactly 500 characters.\n",
        "* **Semantic:** Breaking where a topic ends (paragraphs or sub-headers).\n",
        "* **Overlapping:** Keeping the last 50 tokens of one chunk at the start of the next to maintain context across the breaks.\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Context Optimisation:** Balancing the \"compression ratio\" (how much you shrink it) against the \"accuracy\" (how well the AI still answers).\n",
        "* **Token Efficiency:** Crafting system instructions that force the model to be extremely concise, using shorthand or structured data rather than flowing prose.\n",
        "\n",
        "\n",
        "### Comparison: Standard RAG vs. Compressed RAG\n",
        "\n",
        "| Feature | Standard RAG | Compressed Context |\n",
        "| --- | --- | --- |\n",
        "| **Token Usage** | High (sends full text chunks). | Low (sends only core meaning). |\n",
        "| **Latency** | Slower (more tokens to process). | Faster (shorter input = faster TTFT). |\n",
        "| **Accuracy** | High (full detail available). | Variable (depends on compression quality). |\n",
        "| **Best For** | Fact-heavy, legal, or medical tasks. | High-volume chat, mobile apps, or cost-saving. |\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Semantic Ranking:** Use embeddings to find the top 5 most relevant paragraphs before attempting to compress them.\n",
        "2. **Instruction Density:** Tell the model to use \"Telegram Style\" (e.g., \"Market up 5%. CEO resigned. Stock stable.\").\n",
        "3. **Validation:** Occasionally run a \"loss check\"—ask the model to reconstruct a specific detail from the compressed text to see if it was accidentally deleted."
      ],
      "metadata": {
        "id": "FXT4xwm1fJsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Code Example: Simple LLM-based Compression\n",
        "\n",
        "#This uses a \"Small Model\" (like GPT-4o-mini) to compress context for a \"Big Model\" (like GPT-4o) to save money.\n",
        "\n",
        "\n",
        "def compress_context(long_text):\n",
        "    # Use a cheaper model for the 'Lossy' compression step\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Compress the following text into a dense, bulleted summary. Remove all fluff. Preserve all numbers, names, and key facts.\"\n",
        "        }, {\"role\": \"user\", \"content\": long_text}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# 1. Take a 2000-word document\n",
        "source_text = \"...\"\n",
        "\n",
        "# 2. Compress it to ~200 words\n",
        "compressed_prompt = compress_context(source_text)\n",
        "\n",
        "# 3. Use the 'expensive' model for the final reasoning with the compressed data\n",
        "#final_answer = client.chat.completions.create(model=\"gpt-4o\", prompt=compressed_prompt...)\n"
      ],
      "metadata": {
        "id": "BojwU-wXfqye"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. Prompt Templates\n",
        "\n",
        "**Prompt Templates** are reusable, structured frameworks used to guide the creation of consistent AI inputs. Instead of writing a new prompt from scratch for every request, you define a **\"Master Blueprint\"** that contains fixed instructions and dynamic slots for specific data.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Blueprint for AI:** A static string of text with defined \"holes\" or placeholders that get filled at runtime.\n",
        "* **Consistency Engine:** Ensures that regardless of the specific user input, the model's persona, constraints, and formatting rules remain identical across every call.\n",
        "* **Separation of Concerns:** Decouples the *logic* of the prompt (the template) from the *data* (the user query or context).\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Variable Placeholders:** The syntax used to mark dynamic sections (e.g., `{topic}` in f-strings or `{{user_name}}` in Jinja2). You must learn how to handle different types of variables, such as `UserQuery`, `RetrievedContext` (for RAG), and `SystemInstructions`.\n",
        "* **Prompt Parameterisation:** The process of \"injecting\" real-world values into your templates. This includes managing default values, handling missing parameters, and sanitizing input to prevent prompt injection.\n",
        "* **Version Control:** Storing templates in a central repository (like Git or a Prompt Registry). You must learn to use **Semantic Versioning** (e.g., `v1.2.0`) to ensure that an update to a template doesn't break production applications that rely on the older structure.\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Prompt Standardisation:** Creating a uniform \"style guide\" for prompts across an entire organization. This ensures that every developer uses the same structure for things like **[Persona]**, **[Task]**, and **[Output Format]**.\n",
        "* **Scalability:** Managing a library of hundreds or thousands of prompts. This involves using **Prompt Registries** (like Langfuse or PromptLayer) to dynamically fetch the correct prompt version at runtime based on the task type.\n",
        "\n",
        "\n",
        "### Comparison: Static Prompt vs. Prompt Template\n",
        "\n",
        "| Feature | Static Prompt | Prompt Template |\n",
        "| --- | --- | --- |\n",
        "| **Flexibility** | Zero; hard-coded for one specific task. | High; adaptable to infinite variations. |\n",
        "| **Maintenance** | High; must update every single code instance. | Low; update the template once in a central hub. |\n",
        "| **Error Risk** | High; manual entry leads to typos. | Low; structured slots prevent instruction drift. |\n",
        "| **Performance** | Inconsistent across inputs. | Highly predictable and easier to benchmark. |"
      ],
      "metadata": {
        "id": "y998f7sjelMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. Safety & Guardrail Prompting\n",
        "\n",
        "**Safety and Guardrail Prompting** is the practice of designing instructions and filters that keep an AI model’s behavior within ethical, legal, and operational boundaries. It ensures the model can resist adversarial attacks (jailbreaking) and avoids generating harmful, biased, or restricted content.\n",
        "\n",
        "\n",
        "### What It Is\n",
        "\n",
        "* **Hard Boundaries:** Setting non-negotiable rules that the model cannot override, even if a user explicitly asks it to \"ignore all previous instructions.\"\n",
        "* **Multi-Layered Defense:** Guardrails typically operate at three stages:\n",
        "1. **Input:** Sanitizing and rejecting malicious prompts before the model sees them.\n",
        "2. **In-Process:** Guiding the model's internal reasoning to prioritize safety.\n",
        "3. **Output:** Filtering or redacting the final response before the user sees it.\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "#### 1. Policy-aligned Instructions\n",
        "\n",
        "Learn to translate abstract corporate or legal policies into concrete, unambiguous instructions.\n",
        "\n",
        "* **Vague:** \"Don't say anything bad.\"\n",
        "* **Aligned:** \"You are strictly prohibited from providing medical diagnoses. If asked for health advice, you must recommend consulting a licensed professional and provide a general disclaimer.\"\n",
        "\n",
        "#### 2. Refusal Handling\n",
        "\n",
        "Traditional safety often uses \"Binary Refusal\" (a hard \"No\"), which can be brittle. You must learn to design refusals that are helpful but safe.\n",
        "\n",
        "* **Over-refusal:** Rejecting a benign request because it contains a \"trigger word\" (e.g., refusing to talk about \"killing a computer process\").\n",
        "* **Graceful Refusal:** Explaining *why* a request cannot be fulfilled and pivoting to a safe alternative.\n",
        "\n",
        "#### 3. Safe Completion Design\n",
        "\n",
        "Move beyond just saying \"No\" to **Safe Completions**. This involves teaching the model to answer the \"safe\" parts of a query while ignoring the \"dangerous\" ones.\n",
        "\n",
        "* **Example:** If a user asks, \"How do I hack a Wi-Fi password for educational research?\", a safe completion would explain the *principles* of network security and encryption without providing actionable hacking steps.\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Risk Mitigation:** Identifying \"edge cases\" where a model might fail, such as **Prompt Injections** (manipulating the model into a different persona) or **Data Leakage** (tricking the model into revealing system prompts).\n",
        "* **Ethical Prompting:** Proactively identifying and reducing bias in outputs. This involves using \"Neutral Persona\" instructions and ensuring diverse perspectives are represented in open-ended tasks.\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "* **Use \"Negative Constraints\":** Clearly list what the model should **NOT** do (e.g., \"Do not use technical jargon,\" \"Do not mention competitor X\").\n",
        "* **Red Teaming:** Regularly act as a \"hacker\" to try and break your own prompts to find vulnerabilities.\n",
        "* **The \"Context Sandwich\":** Place your most important safety instructions at both the very beginning and the very end of the prompt to take advantage of \"primacy\" and \"recency\" effects in AI attention.\n"
      ],
      "metadata": {
        "id": "PzMPAKVyd4m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Code Example: The \"Self-Correction\" Guardrail\n",
        "\n",
        "#One effective technique is to have the AI \"think\" about safety before it generates its final answer.\n",
        "\n",
        "# A System Prompt that forces a safety check first\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful assistant. Before answering any query, perform a 1-second internal safety audit.\n",
        "\n",
        "1. Does this request involve illegal acts, PII, or medical advice?\n",
        "2. If YES, state your refusal clearly and suggest a safe alternative.\n",
        "3. If NO, proceed with a helpful answer.\n",
        "\n",
        "Format your response as:\n",
        "<audit> [Pass/Fail + Reason] </audit>\n",
        "<response> [Your actual answer] </response>\n",
        "\"\"\"\n",
        "\n",
        "# Example Usage\n",
        "user_query = \"Tell me how to bypass a neighbor's Wi-Fi.\"\n",
        "# The model would output:\n",
        "# <audit> Fail: Request involves unauthorized access to networks. </audit>\n",
        "# <response> I cannot assist with bypassing network security. However, I can explain how to secure your own Wi-Fi using WPA3 encryption. </response>\n"
      ],
      "metadata": {
        "id": "PkbVPQBCeVlz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. Evaluation-Driven Prompting\n",
        "\n",
        "**Evaluation-Driven Prompting** is the practice of designing prompts specifically to be measured against quantitative and qualitative benchmarks. Instead of relying on \"vibes\" or subjective feel, you treat prompt engineering as an experimental science where every change is validated by data.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Prompt-as-a-Variable:** Treating the prompt text as a variable in a controlled experiment to see which version maximizes a specific score (e.g., Accuracy).\n",
        "* **The \"Ground Truth\" Comparison:** The process of comparing AI outputs against a \"Golden Dataset\" (a set of perfect, human-verified answers).\n",
        "* **Closing the Feedback Loop:** Using metric failures to identify exactly *where* a prompt is failing (e.g., low Recall means the prompt isn't extracting enough information).\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Alignment with Metrics:**\n",
        "  * **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Used for summarization. It measures how much of the \"Gold Standard\" information the AI actually captured.\n",
        "  * **BLEU (Bilingual Evaluation Understudy):** Used for translation and text similarity. It checks for exact word-sequence matches.\n",
        "  * **Accuracy/F1 Score:** Used for classification (e.g., \"Is this sentiment positive or negative?\").\n",
        "\n",
        "\n",
        "* **Prompt Benchmarking:** Creating a \"Golden Dataset\" of 50–100 diverse test cases to run against every new prompt version to ensure no \"regressions\" (where fixing one thing breaks another).\n",
        "* **A/B Prompt Testing:** Running two different prompts (Prompt A vs. Prompt B) on the same dataset to statistically prove which one performs better.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Experimental Design:** Identifying which variable to change (e.g., adding a \"Think step-by-step\" instruction) and keeping everything else constant to measure its true impact.\n",
        "* **Prompt Optimization:** Systematically \"trimming\" and \"tuning\" a prompt to reach the highest possible metric score with the lowest token cost.\n",
        "\n",
        "\n",
        "\n",
        "### Comparison: Traditional vs. Evaluation-Driven\n",
        "\n",
        "| Feature | Traditional Prompting | Evaluation-Driven Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Validation** | \"Looks good to me.\" | ROUGE, BLEU, or Accuracy scores. |\n",
        "| **Testing** | 1–2 manual checks. | Automated testing on 100+ cases. |\n",
        "| **Decision Making** | Intuition-based. | Data-driven (A/B testing). |\n",
        "| **Scalability** | Hard to maintain as logic grows. | Highly stable due to regression testing. |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Use \"LLM-as-a-Judge\":** For creative tasks where word-matching (BLEU) fails, use a more powerful model (like GPT-4o) to \"grade\" the output of a smaller model based on a rubric.\n",
        "2. **Monitor for Regression:** Always run your *entire* benchmark set after a change. A prompt that makes \"Summary A\" better might make \"Summary B\" much worse.\n",
        "3. **Thresholds:** Set a \"Minimum Quality Bar.\" If a prompt change doesn't increase your metric by at least 5%, it might not be worth the added complexity."
      ],
      "metadata": {
        "id": "31ADRE4aiOuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Code Example: Simple A/B Testing with ROUGE\n",
        "\n",
        "#This Python script compares two summary prompts using the `rouge-score` library to see which one better matches a \"Human Reference.\"\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# 1. Define our \"Ground Truth\" (What a perfect summary looks like)\n",
        "reference_text = \"The company's Q3 revenue grew by 15% due to high demand in Asia.\"\n",
        "\n",
        "# 2. Two different outputs from Prompt A and Prompt B\n",
        "output_a = \"Q3 saw a 15% revenue increase, mostly from Asian markets.\"\n",
        "output_b = \"The financial report indicates the company is doing well globally.\"\n",
        "\n",
        "# 3. Setup the Scorer (Focusing on ROUGE-L for sequence overlap)\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "score_a = scorer.score(reference_text, output_a)\n",
        "score_b = scorer.score(reference_text, output_b)\n",
        "\n",
        "print(f\"Prompt A ROUGE-L: {score_a['rougeL'].fmeasure:.4f}\")\n",
        "print(f\"Prompt B ROUGE-L: {score_b['rougeL'].fmeasure:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPl_JFk8iUxP",
        "outputId": "accfb260-164a-468a-fe42-ae61afae272d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt A ROUGE-L: 0.1667\n",
            "Prompt B ROUGE-L: 0.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26. Multimodal Prompting\n",
        "\n",
        "Multimodal Prompting is the practice of providing inputs to an AI model that combine different formats—such as **text, images, audio, and video**—to achieve a single outcome. Instead of just \"reading,\" the model \"sees\" or \"hears\" the context alongside your written instructions.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Sensory Integration:** Giving the AI \"eyes\" and \"ears\" to understand the physical world or digital media.\n",
        "* **Non-Verbal Context:** Using an image to provide information that would be too complex to describe in words (e.g., a hand-drawn diagram or a blurry medical scan).\n",
        "* **Inter-modal Cooperation:** The model uses the text to focus its \"attention\" on specific parts of the visual or auditory input.\n",
        "\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Cross-modal Instruction Clarity:** Learning how to refer to objects within an image or video so the model doesn't get confused (e.g., \"Look at the red graph in the top-right corner\").\n",
        "* **Image Grounding:** Ensuring the model's textual response is strictly based on the visual evidence provided, rather than hallucinating based on general knowledge.\n",
        "* **Modality-specific Constraints:** Understanding the limits of each format. For example, some models can \"see\" text in images (OCR) but cannot accurately judge the exact distance between two objects in a 3D space.\n",
        "\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Multimodal Reasoning:** The ability to draw conclusions by combining a visual clue with a logical rule.\n",
        "* *Example:* Looking at a picture of a wilting plant and a text prompt about its watering schedule to diagnose the problem.\n",
        "\n",
        "\n",
        "* **Instruction Fusion:** Blending a command and a piece of media so they function as a single unit of logic.\n",
        "\n",
        "\n",
        "### Comparison: Text-Only vs. Multimodal Prompting\n",
        "\n",
        "| Feature | Text-Only Prompting | Multimodal Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Input Depth** | Limited by your descriptive ability. | High; captures \"a thousand words\" in one file. |\n",
        "| **Logic** | Relies on linguistic patterns. | Relies on spatial and visual patterns + text. |\n",
        "| **Main Challenge** | Ambiguity in description. | \"Noise\" or clutter in the image/audio. |\n",
        "| **Best For** | Writing, Coding, Summarizing. | Design, Troubleshooting, Medical, Security. |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Reference specific visual areas:** Use terms like \"the foreground,\" \"the text in the bottom left,\" or \"the person wearing the blue shirt.\"\n",
        "2. **Use High Resolution:** If the task involves reading text or small details, the image quality must be high, as models \"downsample\" images during processing.\n",
        "3. **Sequential Multimodal:** For video, describe the timestamps if possible (e.g., \"What happens at 02:30?\")."
      ],
      "metadata": {
        "id": "nDQvzvDGh3Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Code Example: Analyzing an Image with GPT-4o\n",
        "\n",
        "#This Python snippet demonstrates how to send an image (via URL) along with a specific text instruction to perform \"Image Grounding.\"\n",
        "\n",
        "def analyze_fridge_contents(image_url):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"What can I cook with the ingredients you see in this fridge? Provide a recipe.\"},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": image_url},\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Usage:\n",
        "# The model \"sees\" the eggs, milk, and spinach in the photo and suggests an Omelet.\n",
        "img_link = \"data:image/jpeg;base64,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\"\n",
        "print(analyze_fridge_contents(img_link))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTonYJUNh9Up",
        "outputId": "23457975-50a5-459a-a4d7-4d21075189ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can't see the details of the contents in the fridge from the image, but I can help you come up with a recipe if you provide a list of what you have! Here's a simple example using common ingredients:\n",
            "\n",
            "**Vegetable Stir-Fry**\n",
            "\n",
            "**Ingredients:**\n",
            "- Mixed vegetables (e.g., bell peppers, broccoli, carrots)\n",
            "- Onion\n",
            "- Garlic\n",
            "- Soy sauce\n",
            "- Olive oil or vegetable oil\n",
            "- Salt and pepper\n",
            "- Optional: Tofu or chicken\n",
            "\n",
            "**Instructions:**\n",
            "1. **Prepare Vegetables:** Wash and chop the mixed vegetables into bite-sized pieces. Dice the onion and mince the garlic.\n",
            "\n",
            "2. **Cook Protein (Optional):** If using tofu or chicken, cut into cubes. Heat a tablespoon of oil in a pan and cook the protein until browned. Remove and set aside.\n",
            "\n",
            "3. **Sauté Aromatics:** In the same pan, add a little more oil if needed and sauté the onion and garlic until fragrant.\n",
            "\n",
            "4. **Add Vegetables:** Add the vegetables to the pan and stir-fry over medium-high heat until they are tender-crisp.\n",
            "\n",
            "5. **Combine and Season:** Return the protein to the pan. Add soy sauce, salt, and pepper to taste. Stir well to combine.\n",
            "\n",
            "6. **Serve:** Serve hot over cooked rice or noodles if desired.\n",
            "\n",
            "Let me know what's in your fridge or if you have any preferences!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. Agentic Prompting\n",
        "\n",
        "Agentic Prompting is the design and optimization of instructions that enable an AI to act as an **autonomous agent**. Unlike standard prompting, where the AI simply answers a question, Agentic Prompting empowers the AI to set its own sub-goals, use tools, and iterate until a complex objective is met.\n",
        "\n",
        "---\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **From \"Chat\" to \"Do\":** Shifting the AI from a passive responder to an active participant that executes workflows.\n",
        "* **Autonomous Reasoning:** The model doesn't just follow a script; it decides *which* steps to take based on the current situation.\n",
        "* **Closed-Loop Systems:** The AI observes the result of its action (Observation), thinks about it (Thought), and decides the next move.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Goal Decomposition:** Teaching the model to take a \"North Star\" goal (e.g., \"Research and write a report on X\") and break it into a \"Task List\" (1. Search X, 2. Fact-check X, 3. Draft report).\n",
        "* **Memory Handling:** * **Short-term:** Managing the \"context window\" so the agent remembers what it just did.\n",
        "* **Long-term:** Instructing the agent on how to use external databases (RAG) to store and retrieve info across sessions.\n",
        "\n",
        "\n",
        "* **Tool Usage Policies:** Defining strict rules for *when* and *how* an agent can use external APIs (e.g., \"Only use the Python tool if the math is complex\").\n",
        "* **Failure Recovery:** Explicitly prompting the agent on how to handle \"Dead Ends.\" If a tool fails, the agent should have a backup plan rather than getting stuck in a loop.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Agent Architecture Thinking:** Designing the prompt not as a single message, but as a \"System Instruction\" that governs a multi-turn process.\n",
        "* **Autonomy Control:** Striking a balance between giving the agent freedom to solve problems and setting \"Human-in-the-loop\" (HITL) checkpoints for safety.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison: ReAct vs. Agentic Prompting\n",
        "\n",
        "| Feature | ReAct Prompting | Agentic Prompting |\n",
        "| --- | --- | --- |\n",
        "| **Scope** | A single logic loop (Thought/Action). | A long-running, multi-step mission. |\n",
        "| **Initiative** | Guided by specific prompt steps. | High; the agent manages its own task list. |\n",
        "| **Complexity** | Simple tool use (Search, Calc). | Complex workflows (Code, Research, File Ops). |\n",
        "| **Supervision** | Usually direct. | Often operates in the background. |\n",
        "\n",
        "\n",
        "\n",
        "### Best Practices for Agentic Design\n",
        "\n",
        "1. **State Management:** Always ask the agent to print its \"Current Task List\" so you can monitor its progress.\n",
        "2. **Safety Guardrails:** Use **Negative Prompting** to prevent the agent from performing dangerous actions (e.g., \"DO NOT delete files\").\n",
        "3. **Reflection:** Combine this with **Self-Reflection Prompting** so the agent critiques its own sub-task results before moving to the next one.\n"
      ],
      "metadata": {
        "id": "-YjXpcGDhe9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Code Example: Designing an Agentic System Prompt\n",
        "\n",
        "# This is an 'Agentic' system prompt.\n",
        "# It defines a persona that manages its own tasks.\n",
        "AGENT_SYSTEM_PROMPT = \"\"\"\n",
        "You are an Autonomous Research Agent.\n",
        "Your goal is to fulfill the user's request by following these rules:\n",
        "\n",
        "1. DECOMPOSE: Break the request into 3-5 sub-tasks.\n",
        "2. EXECUTE: Perform each task. If you need info, use the 'Search' tool.\n",
        "3. EVALUATE: After each task, check if the result is high quality.\n",
        "4. RECOVER: If a search yields no results, try a different keyword.\n",
        "5. FINISH: Only provide the 'Final Report' once all sub-tasks are complete.\n",
        "\n",
        "Format your internal dialogue as:\n",
        "TASK LIST: [list]\n",
        "CURRENT TASK: [task]\n",
        "THOUGHT: [reasoning]\n",
        "ACTION: [tool_call]\n",
        "\"\"\"\n",
        "\n",
        "def run_mission(objective):\n",
        "    # This represents the 'Autonomy Control' loop\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": AGENT_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": objective}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example: The agent will now decide how to research this without further input.\n",
        "print(run_mission(\"Analyze the 2024 trends in renewable energy and write a summary.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M-c3thmhiKK",
        "outputId": "36b97e5f-aaa1-44e2-8775-785479278596"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TASK LIST: \n",
            "1. Identify the key trends expected in the renewable energy sector for 2024.\n",
            "2. Gather data or reports from reputable sources on these 2024 renewable energy trends.\n",
            "3. Evaluate the information to understand the implications for the energy market and policy.\n",
            "4. Summarize the findings into a concise report highlighting the main trends and their potential impact.\n",
            "\n",
            "CURRENT TASK: Identify the key trends expected in the renewable energy sector for 2024.\n",
            "\n",
            "THOUGHT: To accurately analyze the 2024 trends in renewable energy, I need to first identify what these trends are by examining forecasts or expert analyses.\n",
            "\n",
            "ACTION: Search(\"2024 renewable energy trends forecast\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. Meta-Prompting\n",
        "\n",
        "**Meta-Prompting** is an advanced prompt engineering technique where you use an AI to design, refine, and optimize other prompts. Instead of trying to write the perfect instructions yourself, you act as an \"Architect\" or \"Orchestrator,\" giving the AI high-level goals and letting it handle the low-level prompt engineering.\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **The \"Prompt for Prompts\" Concept:** Shifting from asking the AI to solve a task directly to asking the AI to *create a prompt* that solves the task.\n",
        "* **Prompt-as-a-Product:** Treating prompts as dynamic code that can be generated, tested, and updated automatically by the AI itself.\n",
        "* **Recursive Logic:** Using an AI to analyze its own previous responses to identify where its prompt was ambiguous, then having it rewrite that prompt for a second attempt.\n",
        "\n",
        "### What you MUST learn\n",
        "\n",
        "* **Prompt Evaluation Criteria:** Learning how to teach the AI to judge prompt quality. You must define metrics like *clarity, specificity, constraint adherence,* and *persona consistency* so the AI knows what a \"good\" generated prompt looks like.\n",
        "* **Prompt Generation Heuristics:** The \"rules of thumb\" for prompt creation (e.g., always include a Persona, provide Step-by-Step logic, and specify Output Format). You feed these rules into the Meta-Prompt so the AI follows best practices.\n",
        "* **Self-improving Systems:** Designing feedback loops (like **Conversational Prompt Engineering**) where the AI generates a prompt, runs it, evaluates the result, and modifies the prompt based on failures.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Prompt Abstraction:** The ability to look at a specific task (e.g., \"Summarize this legal contract\") and turn it into a high-level instruction for the AI (e.g., \"Create a specialized legal-document summarizer prompt that extracts key liabilities\").\n",
        "* **Automation of Prompting:** Integrating AI-driven prompt generation into software workflows. Instead of hard-coding 100 different prompts for 100 different tools, you write one Meta-Prompt that generates the correct tool-specific prompt on the fly based on user input.\n",
        "\n",
        "\n",
        "### Example: The \"Prompt Architect\" Meta-Prompt\n",
        "\n",
        "This is a prompt designed to generate a better prompt.\n",
        "\n",
        "**User (Meta-Prompt):**\n",
        "\n",
        "> \"I want you to act as a **Prompt Engineering Expert**. I will give you a simple task, and you will generate a high-quality, structured prompt for an AI to execute that task. Use a persona, clear steps, and a JSON output format.\n",
        "> **Task:** Analyze the sentiment of customer reviews for a coffee shop.\"\n",
        "\n",
        "**AI (Generated Prompt):**\n",
        "\n",
        "> \"You are a **Customer Experience Analyst**. Please analyze the following coffee shop reviews.\n",
        "> **Steps:**\n",
        "> 1. Categorize the review as Positive, Neutral, or Negative.\n",
        "> 2. Identify specific mentions of 'Service', 'Coffee Quality', or 'Atmosphere'.\n",
        "> 3. Provide a 'Fix-it' recommendation for any negative review.\n",
        ">\n",
        ">\n",
        "> **Output Format:** JSON only.\"\n",
        "\n",
        "### Benefits of Meta-Prompting (per GeeksforGeeks)\n",
        "\n",
        "* **Scalability:** Quickly generate prompts for dozens of different domains.\n",
        "* **Reduced Bias:** By focusing on structure and logic, it minimizes the \"fluff\" or accidental bias a human might write into a prompt.\n",
        "* **Efficiency:** It automates the \"trial and error\" phase of prompt engineering.\n"
      ],
      "metadata": {
        "id": "u7hB8Ot4hBLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 29. Prompt Debugging Techniques\n",
        "\n",
        "**Prompt Debugging** is the systematic process of identifying why a model’s output is failing to meet expectations and applying targeted fixes. It moves prompt engineering from \"guessing and checking\" to a rigorous, data-driven optimization process.\n",
        "\n",
        "\n",
        "### What it is\n",
        "\n",
        "* **Troubleshooting Logic:** Isolating whether a failure is due to the prompt, the model's limitations, or the data provided.\n",
        "* **Root Cause Analysis:** Determining if the AI is \"confused\" by wording, overwhelmed by context, or simply lacks the internal knowledge to answer.\n",
        "* **Performance Tuning:** Refining a prompt to be as short, fast, and accurate as possible.\n",
        "\n",
        "### What You MUST Learn\n",
        "\n",
        "* **Identifying Failure Points:** Pinpointing exactly where the chain breaks.\n",
        "* *Examples:* Ambiguous instructions, \"Context Drift\" (losing the plot in long prompts), or \"Instruction Conflict\" (two rules clashing).\n",
        "\n",
        "\n",
        "* **Token Usage Analysis:**\n",
        "* **Context Window Management:** Understanding if important info is being truncated or ignored because it's in the \"middle\" of a long prompt.\n",
        "* **Cost Optimization:** Identifying \"bloat\" text that adds cost without adding accuracy.\n",
        "\n",
        "\n",
        "* **Sensitivity Testing (Robustness):** Testing how small changes (e.g., swapping \"Summarize\" for \"Briefly explain\") affect the output. A \"fragile\" prompt that breaks with one word change is a debugging red flag.\n",
        "\n",
        "### Key Skills\n",
        "\n",
        "* **Prompt Diagnostics:** Using logs and versioning to track which specific prompt version caused which error.\n",
        "* **Iterative Debugging:** The \"Scientific Method\" applied to AI—changing one variable at a time, testing, and recording results.\n",
        "\n",
        "\n",
        "### Comparison: Trial & Error vs. Systematic Debugging\n",
        "\n",
        "| Feature | Trial & Error | Systematic Debugging |\n",
        "| --- | --- | --- |\n",
        "| **Approach** | Randomly changing words. | Changing one variable at a time. |\n",
        "| **Tracking** | \"In your head.\" | Documented version logs / A/B testing. |\n",
        "| **Speed** | Fast initially, slow to solve deep issues. | Slower start, much faster to reach 99% accuracy. |\n",
        "| **Scalability** | Not scalable. | Can be automated with scripts. |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Debugging Pro-Tips\n",
        "\n",
        "1. **Isolation:** If a prompt is long and failing, delete half of it. If it still fails, the error is in the remaining half. If it works, the error was in the part you deleted.\n",
        "2. **Log Token Counts:** Always track `usage.total_tokens` in your API calls. If a prompt suddenly gets much more expensive, you might have a \"looping\" or \"hallucination\" bug.\n",
        "3. **The \"Golden Dataset\":** Keep a list of 10-20 \"hard\" questions that your model *must* get right. Every time you change your prompt, run it against this \"Golden Set\" to ensure you didn't break something else (Regression Testing).\n"
      ],
      "metadata": {
        "id": "RpWQmEtzgaO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Code Example: A/B Testing for Debugging\n",
        "\n",
        "# PROMPT A: Vague (Likely to fail)\n",
        "prompt_a = \"Tell me about the financial report.\"\n",
        "\n",
        "# PROMPT B: Debugged (Specific & Grounded)\n",
        "prompt_b = \"Summarize the 'Q3 Revenue' section of this report. Focus only on year-over-year growth percentages. Format as a list.\"\n",
        "\n",
        "def debug_compare(test_data):\n",
        "    prompts = [prompt_a, prompt_b]\n",
        "    results = []\n",
        "\n",
        "    for i, p in enumerate(prompts):\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"{p}\\nData: {test_data}\"}],\n",
        "            temperature=0\n",
        "        )\n",
        "        results.append(f\"Version {chr(65+i)}: {response.choices[0].message.content}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Sample 'Test Data' that might be confusing\n",
        "report_data = \"Q3 Revenue: $50M (Up 10% from last year). Total staff: 500.\"\n",
        "for res in debug_compare(report_data):\n",
        "    print(res + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyaJAUzSgh0r",
        "outputId": "d8c30d29-f002-461b-a2d6-bb9dbcc383a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version A: The financial report for the third quarter indicates a positive performance for the company. The revenue for Q3 stands at $50 million, which represents a 10% increase compared to the same period last year. This growth suggests that the company has been successful in enhancing its sales or expanding its market presence.\n",
            "\n",
            "Additionally, the company maintains a total staff of 500 employees. This figure can provide insights into the company's operational scale and its capacity to manage increased business activities. The stable or growing workforce might also reflect the company's confidence in its future growth prospects.\n",
            "\n",
            "Overall, the increase in revenue is a positive indicator of the company's financial health and operational efficiency. It would be beneficial to analyze further details such as profit margins, cost management, and market conditions to gain a comprehensive understanding of the company's performance.\n",
            "\n",
            "Version B: - Q3 Revenue increased by 10% year-over-year.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30. Prompt Governance & Versioning\n",
        "## 1. What It Is  \n",
        "### Managing Prompts at Scale\n",
        "\n",
        "- **Prompt Governance & Versioning** is the discipline of **managing prompts systematically across teams, applications, and time**.\n",
        "- Prompts are treated as **first-class production assets**, similar to:\n",
        "  - Source code\n",
        "  - Configuration files\n",
        "  - ML models\n",
        "# Prompts are:\n",
        "  - Centrally stored\n",
        "  - Versioned\n",
        "  - Dynamically retrieved\n",
        "  - Monitored and improved continuously\n",
        "- This avoids:\n",
        "  - Hard-coded prompts\n",
        "  - Untracked edits\n",
        "  - Unexplainable model behavior\n",
        "\n",
        "> Core idea: **Prompts control AI behavior — managing them poorly means managing AI poorly.**\n",
        "\n",
        "\n",
        "## 2. Why Managing Prompts at Scale Matters\n",
        "\n",
        "- Small prompt changes can cause:\n",
        "  - Major output differences\n",
        "  - Quality regressions\n",
        "  - Safety or compliance issues\n",
        "- At scale, organizations must know:\n",
        "  - Which prompt version is in production\n",
        "  - How it evolved\n",
        "  - Who approved it\n",
        "  - How it performs over time\n",
        "- Managing prompts at scale enables:\n",
        "  - Safe iteration\n",
        "  - Collaboration\n",
        "  - Faster experimentation\n",
        "  - Reliable deployments\n",
        "\n",
        "\n",
        "## 3. What You MUST Learn\n",
        "\n",
        "### 3.1 Prompt Version Control\n",
        "\n",
        "- Every prompt change creates a **new immutable version**\n",
        "- Older versions are **never overwritten**\n",
        "- Use **semantic versioning**:\n",
        "  - `v1.0` – initial version\n",
        "  - `v1.1` – small wording change\n",
        "  - `v2.0` – behavioral change\n",
        "- Prompts may have environment labels:\n",
        "  - `draft`\n",
        "  - `staging`\n",
        "  - `production`\n",
        "  - `deprecated`\n",
        "\n",
        "**Why it matters**\n",
        "- Enables rollback\n",
        "- Supports A/B testing\n",
        "- Prevents silent regressions\n",
        "- Makes experimentation safe\n",
        "\n",
        "> Rule: *If the prompt changed, the version must change.*\n",
        "\n",
        "### 3.2 Audit Logs\n",
        "\n",
        "- Audit logs record:\n",
        "  - Who changed a prompt\n",
        "  - When it was changed\n",
        "  - What changed\n",
        "  - Which application or agent used it\n",
        "  - Which outputs were produced\n",
        "- Logs typically capture:\n",
        "  - Prompt ID & version\n",
        "  - Model used\n",
        "  - Parameters (temperature, etc.)\n",
        "  - Timestamp\n",
        "  - Metadata (use case, environment)\n",
        "\n",
        "**Why it matters**\n",
        "- Debugging unexpected outputs\n",
        "- Root-cause analysis\n",
        "- Accountability\n",
        "- Regulatory and internal audits\n",
        "\n",
        "> If you cannot trace behavior back to a prompt version, the system is not enterprise-ready.\n",
        "\n",
        "\n",
        "### 3.3 Reproducibility\n",
        "\n",
        "- Reproducibility means the ability to **recreate past behavior**\n",
        "- Requires tracking:\n",
        "  - Prompt version\n",
        "  - Model version\n",
        "  - Inference parameters\n",
        "- Enables:\n",
        "  - Debugging failures\n",
        "  - Performance comparison\n",
        "  - Legal defensibility\n",
        "  - Scientific evaluation\n",
        "\n",
        "> Same input + same prompt version + same model config = predictable behavior.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Prompt Lifecycle (Like Software)\n",
        "\n",
        "Prompts follow a lifecycle similar to traditional software:\n",
        "\n",
        "1. **Design & Ideation**\n",
        "   - Write or modify prompt text\n",
        "   - Decide parameters\n",
        "   - Save as a new version\n",
        "\n",
        "2. **Experimentation & Testing**\n",
        "   - Compare variants\n",
        "   - Store results with version metadata\n",
        "\n",
        "3. **Deployment**\n",
        "   - Mark a prompt as “production”\n",
        "   - Applications fetch it dynamically\n",
        "\n",
        "4. **Monitoring & Feedback**\n",
        "   - Track quality, cost, latency\n",
        "   - Collect user feedback\n",
        "\n",
        "5. **Refinement**\n",
        "   - Improve prompt\n",
        "   - Release a new version\n",
        "\n",
        "> Prompt iteration mirrors agile development.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Key Skills\n",
        "\n",
        "### Enterprise Prompt Management\n",
        "\n",
        "- Centralized prompt registry (no hardcoding)\n",
        "- Metadata for each prompt:\n",
        "  - Owner\n",
        "  - Version\n",
        "  - Status\n",
        "  - Intended use case\n",
        "- Approval workflows before production use\n",
        "- CI/CD-like validation for prompt changes\n",
        "- Feature flags for safe rollout and rollback\n",
        "- Collaboration between:\n",
        "  - Engineers\n",
        "  - Product managers\n",
        "  - Domain experts\n",
        "\n",
        "> Writing prompts is a skill.  \n",
        "> Managing prompts at scale is an **engineering discipline**.\n",
        "\n",
        "\n",
        "\n",
        "## 6. Tools for Prompt Management & Versioning\n",
        "\n",
        "### Langfuse\n",
        "- Open-source LLM observability and prompt management\n",
        "- Prompt versioning with labels (production/staging)\n",
        "- A/B testing and experiments\n",
        "- Tracing, latency, cost, and token metrics\n",
        "- Integration with LLM frameworks\n",
        "\n",
        "### OpenPipe\n",
        "- Logs prompt–completion pairs\n",
        "- Builds datasets from real usage\n",
        "- Supports evaluation and fine-tuning workflows\n",
        "- Focuses on data collection and iteration\n",
        "\n",
        "### Phoenix (Arize)\n",
        "- Combines prompt management with LLM observability\n",
        "- Central Prompt Hub\n",
        "- Version control and rollback\n",
        "- Debugging and trace visualization\n",
        "\n",
        "### LangChain + LangSmith\n",
        "- LangChain: framework for building LLM apps\n",
        "- LangSmith: tracing, evaluation, and prompt versioning\n",
        "- Strong integration for chain-based applications\n",
        "\n",
        "> Tool choice depends on whether you prioritize CMS, observability, experimentation, or integration.\n",
        "\n",
        "\n",
        "\n",
        "## 7. Observability & Evaluation\n",
        "\n",
        "- Track metrics per prompt version:\n",
        "  - Output quality\n",
        "  - Latency\n",
        "  - Cost\n",
        "  - User feedback\n",
        "- Compare versions over time\n",
        "- Detect regressions early\n",
        "- Use logged data to guide improvements\n",
        "\n",
        "\n",
        "\n",
        "## 8. Compliance Readiness\n",
        "\n",
        "### What Compliance Means for Prompts\n",
        "\n",
        "- Prompts must:\n",
        "  - Follow legal requirements\n",
        "  - Respect company policies\n",
        "  - Avoid unsafe or biased instructions\n",
        "- Governance provides:\n",
        "  - Clear audit trails\n",
        "  - Approval records\n",
        "  - Change history\n",
        "  - Controlled rollout and rollback\n",
        "\n",
        "**Critical for**\n",
        "- Healthcare\n",
        "- Finance\n",
        "- Legal\n",
        "- HR\n",
        "- Government systems\n",
        "\n",
        "> Compliance is about **provable control**, not intention.\n",
        "\n",
        "\n",
        "\n",
        "## 9. Practical Use Cases\n",
        "\n",
        "- A/B testing prompt variants\n",
        "- Debugging incorrect or unsafe outputs\n",
        "- Monitoring performance drift\n",
        "- Updating AI behavior without redeploying code\n",
        "- Managing prompts in agentic systems\n",
        "\n"
      ],
      "metadata": {
        "id": "dwRu7k4lGvsk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsR4Y2HfFPaR"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}